<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>pytorch中如何对tensor进行取整操作且梯度不变为0? - @Thoughts Memo</title>
<meta charset="UTF-8">
<meta property="og:type" content="website">
<meta property="og:title" content="pytorch中如何对tensor进行取整操作且梯度不变为0? - @Thoughts Memo">
<meta property="og:site_name" content="ZhiHu Archive for Thoughts Memo">
<meta property="og:url" content="https://www.zhihu.com/question/424790820/answer/2999895083">
<meta name="description" property="og:description" content="不论是取整，还是四舍五入，都有个近似的方法，写个函数： def sin_round(x): return x - torch.sin(2 * np.pi * x) / (2 * np.pi)这个函数的图像如下： [图片] 如果嫌套一层不够的话，可以多叠几次： [图片] 这个是近似四舍五入。要向下取整，就平移一下： [图片] 并且因为用的 sin 函数是可导的，所以操作完梯度都是有的。">
	<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/yue.css@0.4.0/yue.css">
<meta property="twitter:card" content="summary">
<meta name="twitter:title" property="og:title" itemprop="name" content="pytorch中如何对tensor进行取整操作且梯度不变为0? - @Thoughts Memo">
<meta name="twitter:description" property="og:description" itemprop="description" content="不论是取整，还是四舍五入，都有个近似的方法，写个函数： def sin_round(x): return x - torch.sin(2 * np.pi * x) / (2 * np.pi)这个函数的图像如下： [图片] 如果嫌套一层不够的话，可以多叠几次： [图片] 这个是近似四舍五入。要向下取整，就平移一下： [图片] 并且因为用的 sin 函数是可导的，所以操作完梯度都是有的。">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no">
<meta name="google-site-verification" content="U7ZAFUgGNK60mmMqaRygg5vy-k8pwbPbDFXNjDCu7Xk" />
<link rel="alternate" type="application/rss+xml" title="ZhiHu Archive for Thoughts Memo" href="https://l-m-sherlock.github.io/ZhiHuArchive/feed.xml">
<script>
</script>
<style>
img {
vertical-align: middle;
}
figure img {
width: 100%;
}
figure {
margin: 1.4em 0;
}
.author {
display: flex;
gap: 1em;
}
#avatar {
width: 100px;
height: 100px;
}
.author > div {
flex: 1;
}
a[data-draft-type="link-card"] {
   display: block;
}
</style>
</head>
<body style="max-width: 1000px; margin: 0 auto; padding: 0 1em 0 1em;" class="yue">
<p><a href="./">← 返回目录</a></p>
<hr>
<header>
<h1><a href="https://www.zhihu.com/question/424790820/answer/2999895083">pytorch中如何对tensor进行取整操作且梯度不变为0?</a></h1>
<div class="author">
<img class="avatar" id="avatar" src="https://picx.zhimg.com/v2-f958f2b875b0cf4d7ee853e4446ba2d1_l.jpg?source=2c26e567" />
<div>
<h2 rel="author">
<a href="https://api.zhihu.com/people/4c592f496dc33822b560b382907ff1d0" target="_blank">@Thoughts Memo</a>
</h2>
<p> 学校≠教育≠技能；文凭溢价=80%信号传递+20%人力资本 </p>
</div>
</div>
<time datetime="2023-04-25T03:56:02">发表于 2023年04月25日</time>
<p rel="stats"style="color: #999; font-size: 0.9em;">45 👍 / 13 💬</p>
</header>
<article>
<div style="margin: 0; padding: 0.5em 1em; border-left: 4px solid #999; font-size: 0.86em; background: #f9f9f9;">
<h2>问题描述</h2>

</div>
<hr>
<p data-pid="_47TT-ZI">不论是取整，还是四舍五入，都有个近似的方法，写个函数：</p><div class="highlight"><pre><code class="language-python3"><span class="k">def</span> <span class="nf">sin_round</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="k">return</span> <span class="n">x</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span></code></pre></div><p data-pid="0cFUNJY9">这个函数的图像如下：</p><figure data-size="normal"><noscript><img class="origin_image zh-lightbox-thumb" data-caption="" data-default-watermark-src="https://pic1.zhimg.com/50/v2-8e7bbc5489e9bd25e3cb14022eee2507_720w.jpg?source=2c26e567" data-original="https://picx.zhimg.com/v2-155d35d8228c7b9233a1c3aec74cabc6_r.jpg?source=2c26e567" data-original-token="v2-155d35d8228c7b9233a1c3aec74cabc6" data-rawheight="637" data-rawwidth="638" data-size="normal" src="https://pic1.zhimg.com/50/v2-155d35d8228c7b9233a1c3aec74cabc6_720w.jpg?source=2c26e567" width="638"/></noscript><img class="origin_image zh-lightbox-thumb lazy" data-caption="" data-default-watermark-src="https://pic1.zhimg.com/50/v2-8e7bbc5489e9bd25e3cb14022eee2507_720w.jpg?source=2c26e567" data-original="https://picx.zhimg.com/v2-155d35d8228c7b9233a1c3aec74cabc6_r.jpg?source=2c26e567" data-original-token="v2-155d35d8228c7b9233a1c3aec74cabc6" data-rawheight="637" data-rawwidth="638" data-size="normal" src="https://pic1.zhimg.com/50/v2-155d35d8228c7b9233a1c3aec74cabc6_720w.jpg?source=2c26e567" width="638"/></figure><p data-pid="4j4k3H8x">如果嫌套一层不够的话，可以多叠几次：</p><figure data-size="normal"><noscript><img class="origin_image zh-lightbox-thumb" data-caption="" data-default-watermark-src="https://picx.zhimg.com/50/v2-4db3ec2c8a36fc56e81cdf8ffb2b94da_720w.jpg?source=2c26e567" data-original="https://pic1.zhimg.com/v2-111f10b8f4268f03f121d22133506544_r.jpg?source=2c26e567" data-original-token="v2-111f10b8f4268f03f121d22133506544" data-rawheight="811" data-rawwidth="1137" data-size="normal" src="https://picx.zhimg.com/50/v2-111f10b8f4268f03f121d22133506544_720w.jpg?source=2c26e567" width="1137"/></noscript><img class="origin_image zh-lightbox-thumb lazy" data-caption="" data-default-watermark-src="https://picx.zhimg.com/50/v2-4db3ec2c8a36fc56e81cdf8ffb2b94da_720w.jpg?source=2c26e567" data-original="https://pic1.zhimg.com/v2-111f10b8f4268f03f121d22133506544_r.jpg?source=2c26e567" data-original-token="v2-111f10b8f4268f03f121d22133506544" data-rawheight="811" data-rawwidth="1137" data-size="normal" src="https://picx.zhimg.com/50/v2-111f10b8f4268f03f121d22133506544_720w.jpg?source=2c26e567" width="1137"/></figure><p data-pid="ZCD4gwu4">这个是近似四舍五入。要向下取整，就平移一下：</p><figure data-size="normal"><noscript><img class="origin_image zh-lightbox-thumb" data-caption="" data-default-watermark-src="https://picx.zhimg.com/50/v2-7be0cef705a8ca06fc3455e106ea1294_720w.jpg?source=2c26e567" data-original="https://pic1.zhimg.com/v2-563f6f73212f237b34d7b613d63f0f94_r.jpg?source=2c26e567" data-original-token="v2-563f6f73212f237b34d7b613d63f0f94" data-rawheight="732" data-rawwidth="1100" data-size="normal" src="https://pica.zhimg.com/50/v2-563f6f73212f237b34d7b613d63f0f94_720w.jpg?source=2c26e567" width="1100"/></noscript><img class="origin_image zh-lightbox-thumb lazy" data-caption="" data-default-watermark-src="https://picx.zhimg.com/50/v2-7be0cef705a8ca06fc3455e106ea1294_720w.jpg?source=2c26e567" data-original="https://pic1.zhimg.com/v2-563f6f73212f237b34d7b613d63f0f94_r.jpg?source=2c26e567" data-original-token="v2-563f6f73212f237b34d7b613d63f0f94" data-rawheight="732" data-rawwidth="1100" data-size="normal" src="https://pica.zhimg.com/50/v2-563f6f73212f237b34d7b613d63f0f94_720w.jpg?source=2c26e567" width="1100"/></figure><p data-pid="QD6MXCzU">并且因为用的 <code>sin</code> 函数是可导的，所以操作完梯度都是有的。</p>

<hr>
<p><a href="./">← 返回目录</a></p>
</article>
<script src="https://giscus.app/client.js"
data-repo="L-M-Sherlock/ZhiHuArchive"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNDk5NDE0MzM="
data-category="Announcements"
data-category-id="DIC_kwDOFNuuuc4Ck92x"
data-mapping="title"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="0"
data-input-position="top"
data-theme="preferred_color_scheme"
data-lang="zh-CN"
data-loading="lazy"
crossorigin="anonymous"
async>
</script>
</body>
</html>