<!DOCTYPE html>
<html lang="zh-CN">
<head>
<title>06 1989: SuperMemo 适应用户记忆(下) - @Thoughts Memo</title>
<meta charset="UTF-8">
<meta property="og:type" content="website">
<meta property="og:title" content="06 1989: SuperMemo 适应用户记忆(下) - @Thoughts Memo">
<meta property="og:site_name" content="ZhiHu Archive for Thoughts Memo">
<meta property="og:url" content="https://zhuanlan.zhihu.com/p/213170503">
<meta name="description" property="og:description" content="总目录：0 目录《间隔重复的历史》 上一章：06 1989: SuperMemo 适应用户记忆(中) 下…">
<meta property="twitter:card" content="summary">
<meta name="twitter:title" property="og:title" itemprop="name" content="06 1989: SuperMemo 适应用户记忆(下) - @Thoughts Memo">
<meta name="twitter:description" property="og:description" itemprop="description" content="总目录：0 目录《间隔重复的历史》 上一章：06 1989: SuperMemo 适应用户记忆(中) 下…">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no">
<meta name="google-site-verification" content="U7ZAFUgGNK60mmMqaRygg5vy-k8pwbPbDFXNjDCu7Xk" />
<link rel="alternate" type="application/rss+xml" title="ZhiHu Archive for Thoughts Memo" href="https://l-m-sherlock.github.io/ZhiHuArchive/feed.xml">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/yue.css@0.4.0/yue.css">
<script>
</script>
<style>
.origin_image {
width: 100%;
}
figure {
margin:1.4em 0;
}
figure img {
width: 100%;
}
img {
vertical-align: middle;
}
.author {
display: flex;
gap: 1em;
}
#avatar {
width: 100px;
height: 100px;
}
.author > div {
flex: 1;
}
a[data-draft-type="link-card"] {
   display: block;
}
</style>
</head>
<body style="max-width: 1000px; margin: 0 auto; padding: 0 1em 0 1em;" class="yue">
<p><a href="./">← 返回目录</a></p>
<hr>
<header>
<img class="origin_image" src="https://pic1.zhimg.com/v2-a76183f943e37f785644a1ecce4458d5_720w.jpg?source=b1748391"/>
<h1><a href="https://zhuanlan.zhihu.com/p/213170503">06 1989: SuperMemo 适应用户记忆(下)</a></h1>
<div class="author">
<img class="avatar" id="avatar" src="https://picx.zhimg.com/50/v2-f958f2b875b0cf4d7ee853e4446ba2d1_l.jpg?source=b1748391" />
<div>
<h2 rel="author">
<a href="https://api.zhihu.com/people/4c592f496dc33822b560b382907ff1d0" target="_blank">@Thoughts Memo</a>
</h2>
<p> 学校≠教育≠技能；文凭溢价=80%信号传递+20%人力资本 </p>
</div>
</div>
<time datetime="2020-09-02T15:53:52">发表于 2020年09月02日</time>
<p rel="stats"style="color: #999; font-size: 0.9em;">30 👍 / 2 💬</p>
</header>
<article>
<p data-pid="hNbnflxd">总目录：<a class="internal" href="https://zhuanlan.zhihu.com/p/375379522?">0 目录《间隔重复的历史》</a></p><p data-pid="k79ydgMM">上一章：<a class="internal" href="./207094393.html">06 1989: SuperMemo 适应用户记忆(中)</a></p><p data-pid="W0QRLNLI">下篇主要从数据和理论上证明了 SM-5 算法对 SM-2 算法的优越性。本来还有后续的算法优化内容，介于其过于硬核，就不搬到知乎上了，感兴趣的朋友可以直接看我的电子书：</p><a class="wrap external" data-draft-node="block" data-draft-type="link-card" href="https://www.kancloud.cn/ankigaokao/supermemo-guru-cn/1895505" rel="nofollow noreferrer" target="_blank">间隔重复的历史</a><hr/><h3><b>SuperMemo5 的评估（1989）</b></h3><p data-pid="gaAajYJ4">SuperMemo 5的优势如此明显，以至于我没有收集太多数据来证明我的观点。我只拿我的<a class="wrap external" href="https://supermemo.guru/wiki/Master%27s_thesis" rel="nofollow noreferrer" target="_blank">硕士论文</a>做了几次比较，结果毫无疑问。</p><p data-pid="z9XU2C5U">档案警告：<a class="wrap external" href="https://supermemo.guru/wiki/Why_use_literal_archives%3F" rel="nofollow noreferrer" target="_blank">为什么使用文字档案?</a></p><p data-pid="x_9ib961">本文是<a class="wrap external" href="https://supermemo.guru/wiki/Piotr_Wozniak" rel="nofollow noreferrer" target="_blank">Piotr Wozniak</a>(1990)的《<i>优化学习</i>》的一部分。</p><p data-pid="-ThmeRlP"><b>3.8. 算法 SM-5 的评估</b></p><p data-pid="hjCYr5aj">算法 SM-5 自 1989 年 10 月 17 日开始使用，它提供了一种确定理想的最优间隔函数的有效方法，超出了所有人的预期，从而提高了习得率(在 9 个月内学习了 15,000 个项目)。图 3.5 显示习得速率至少是组合应用 SM-2 和 SM-4 算法的两倍！</p><figure data-size="normal"><img class="content_image" data-caption="" data-original-token="v2-02768f3488716d0ea1fe6a5ed5025521" data-rawheight="600" data-rawwidth="407" data-size="normal" src="https://picx.zhimg.com/v2-8c281fa742eee9a032df9a090587ab6d_b.jpg" width="407"/></figure><blockquote data-pid="6KTVejHH"><b>图片：</b>在 SM-2 和 SM-5 算法的监督下，数据库中工作负担的变化</blockquote><p data-pid="_E_M2CEb">对于 10 个月的数据库，知识保留率提高到 96% 左右。下面列出了选定数据库中的一些知识保留率数据，以显示 SM-2 和 SM-5 算法之间的比较：</p><ul><li data-pid="uNSCETjC">日期 - 测量日期，</li><li data-pid="lCOxBv9o">数据库 - 数据库的名称；ALL 表示所有数据库的平均值</li><li data-pid="Bf_jwiDC">间隔 - 数据库中项目使用的平均当前间隔</li><li data-pid="1Pd8nDpj">保留率 - 数据库中的知识保留率</li><li data-pid="8yFZpnyN">版本 - 数据库所应用的算法版本</li></ul><table data-draft-node="block" data-draft-type="table" data-row-style="normal" data-size="normal"><tbody><tr><th>日期</th><th>数据库</th><th>间隔</th><th>保留率</th><th>版本</th></tr><tr><td>Dec 88</td><td>EVD</td><td>17 days</td><td>81%</td><td>SM-2</td></tr><tr><td>Dec 89</td><td>EVG</td><td>19 days</td><td>82%</td><td>SM-5</td></tr><tr><td>Dec 88</td><td>EVC</td><td>41 days</td><td>95%</td><td>SM-2</td></tr><tr><td>Dec 89</td><td>EVF</td><td>47 days</td><td>95%</td><td>SM-5</td></tr><tr><td>Dec 88</td><td>ALL</td><td>86 days</td><td>89%</td><td>SM-2</td></tr><tr><td>Dec 89</td><td>ALL</td><td>190 days</td><td>92%</td><td>SM-2, SM-4 and SM-5</td></tr></tbody></table><p data-pid="lTahNZRs">在复习过程中，记录了下列成绩分布情况:</p><table data-draft-node="block" data-draft-type="table" data-row-style="normal" data-size="normal"><tbody><tr><th>Quality</th><th>Fraction</th></tr><tr><td>0</td><td>0%</td></tr><tr><td>1</td><td>0%</td></tr><tr><td>2</td><td>11%</td></tr><tr><td>3</td><td>18%</td></tr><tr><td>4</td><td>26%</td></tr><tr><td>5</td><td>45%</td></tr></tbody></table><p data-pid="dP5Iz5Kl">根据算法 SM-5 的假设，该分布产生的平均回忆质量等于 4。遗忘指数等于 11%（成绩低于 3 的项目被认为是被遗忘的）。请注意，保留率数据表明数据库中只有 4% 的项目没有被记住。因此，遗忘指数超过遗忘项目百分比的 2.7 倍。</p><p data-pid="rHO-3XSr">在一个 7 个月前的数据库中，发现 70% 的项目在测量之前的重复过程中甚至没有忘记一次，而只有 2% 的项目遗忘次数超过 3 次</p><h3><b>新算法优越性的理论证明</b></h3><p data-pid="W2h6Z4Af">Anki 对 SuperMemo 5 的批评需要根据现代的<a class="wrap external" href="https://supermemo.guru/wiki/Spaced_repetition" rel="nofollow noreferrer" target="_blank">间隔重复</a>理论来做一个简单的证明。我们可以表明，今天的记忆模型可以映射到两种算法基础上的模型：<a class="wrap external" href="https://supermemo.guru/wiki/Algorithm_SM-2" rel="nofollow noreferrer" target="_blank">算法 SM-2</a> 和<a class="wrap external" href="https://supermemo.guru/wiki/Algorithm_SM-5" rel="nofollow noreferrer" target="_blank">算法 SM-5</a>，两者之间的关键区别是最优间隔函数的适应性缺失(在算法SM-5中由最优因子矩阵表示)。</p><p data-pid="-DxQ7HZ3">SInc = f (C、S、R) 是一个<a class="wrap external" href="https://supermemo.guru/wiki/Stability_increase" rel="nofollow noreferrer" target="_blank">稳定性增长</a>函数，<a class="wrap external" href="https://supermemo.guru/wiki/Complexity" rel="nofollow noreferrer" target="_blank">复杂性</a> C、<a class="wrap external" href="https://supermemo.guru/wiki/Stability" rel="nofollow noreferrer" target="_blank">稳定</a> S、<a class="wrap external" href="https://supermemo.guru/wiki/Retrievability" rel="nofollow noreferrer" target="_blank">可恢复性</a> R 作为参数。这个函数决定了最优学习中复习<a class="wrap external" href="https://supermemo.guru/wiki/Interval" rel="nofollow noreferrer" target="_blank">间隔</a>的递增。</p><p data-pid="kQljPUfs">两种算法，SM-2 和 SM-5 都忽略了<a class="wrap external" href="https://supermemo.guru/wiki/Retrievability" rel="nofollow noreferrer" target="_blank">可提取性</a>维度。理论上，如果两种算法都能完美运行，我们可以假设它们的目标是 R=0.9。正如可以在 <a class="wrap external" href="https://supermemo.guru/wiki/SuperMemo" rel="nofollow noreferrer" target="_blank">SuperMemo</a> 中测量的那样，这两种算法都失败了，因为它们不知道相关的<a class="wrap external" href="https://supermemo.guru/wiki/Forgetting_curve" rel="nofollow noreferrer" target="_blank">遗忘曲线</a>。他们只是不收集遗忘曲线数据。这种数据收集的可能性是在 1991 年才在<a class="wrap external" href="https://supermemo.guru/wiki/Algorithm_SM-6" rel="nofollow noreferrer" target="_blank">算法 SM-6</a> 中引入的。</p><p data-pid="RuOUIKk1">然而，如果我们假设 <a class="wrap external" href="https://supermemo.guru/wiki/The_birthday_of_spaced_repetition:_July_31%EF%BC%8C_1985" rel="nofollow noreferrer" target="_blank">1985</a> 和 <a class="wrap external" href="https://supermemo.guru/wiki/SuperMemo_1.0_for_DOS_(1987)" rel="nofollow noreferrer" target="_blank">1987</a> 启发式是完美的猜测，在理论上，该算法可以使用SInc=F(C,S)，常数R为90%。</p><p data-pid="VcZFGqvA">由于 SM-2 使用相同的数字 EF 用于稳定性增加和项目复杂性，对于 SM-2，我们有以 EF=f'(EF,interval) 的形式表示的 SInc=f(C,S) 方程，其中的数据可以很容易地显示 f&lt;&gt;f' 。令人惊讶的是，SM-2 中使用的启发式通过解耦 EF 和项目复杂性之间的实际联系，使这个函数发挥作用。由于数据显示 SInc 随着 S 的增加而不断减少，在算法 SM-2 中，根据定义，如果要用 EF 来表示项的复杂度，那么所有项都需要在每次复习时获得复杂度。在实际应用中，算法 SM-2 使用 EF=f'(EF,interval)，即 SInc(n)=f(SInc(n-1),interval)。</p><p data-pid="rAAHBh3A">让我们假设 EF=f(EF,interval) 启发式正如 SM-2 算法的支持者所声称的那样，是一个很好的猜想。令 <i>SInc</i> 在算法 SM-5 中由 <a class="wrap external" href="https://supermemo.guru/wiki/O-factor" rel="nofollow noreferrer" target="_blank">O-factor</a> 表示。然后我们可以将 <i>SInc</i>=f(C,S) 表示为 <i>OF</i>=f(<a class="wrap external" href="https://supermemo.guru/wiki/EF" rel="nofollow noreferrer" target="_blank">EF</a>,<a class="wrap external" href="https://supermemo.guru/wiki/Interval" rel="nofollow noreferrer" target="_blank">interval</a>)。</p><p data-pid="-IG-a_rG">对于算法 SM-2, OF 是常数，等于 EF，在算法 SM-5 中，OF是可适应的，可以根据算法的表现进行修改。很明显，对表现差的算法进行惩罚，降低<a class="wrap external" href="https://supermemo.guru/wiki/OF_matrix" rel="nofollow noreferrer" target="_blank">OF 矩阵</a>对应的项，并通过增加对应的项来奖励它，这要优于保持不变。</p><p data-pid="UIkT_m1H">有趣的是，尽管 SM-2 算法的支持者声称它表现得很好，但神经网络 SuperMemo 算法的支持者却不断指责代数算法：缺乏适应性。实际上，<a class="wrap external" href="https://supermemo.guru/wiki/Algorithm_SM-17" rel="nofollow noreferrer" target="_blank">算法 SM-17</a> 的适应性是最好的，因为它是基于最精确的<a class="wrap external" href="https://supermemo.guru/wiki/Three_component_model_of_memory" rel="nofollow noreferrer" target="_blank">记忆模型</a>。</p><p data-pid="Qyq8QG8q">可以想象，在 SM-2 中使用的启发法是如此精确，以致于原来对 <i>OF</i>=f(EF,interval) 的猜测不需要修改。然而，正如在实际应用中所显示的那样，<a class="wrap external" href="https://supermemo.guru/wiki/OF_matrix" rel="nofollow noreferrer" target="_blank">OF 矩阵</a>迅速发展，并收敛到<a class="wrap external" href="https://supermemo.guru/wiki/ANE1994" rel="nofollow noreferrer" target="_blank">这篇出版物(Wozniak, Gorzelanczyk 1994)</a>中描述的值。它们与算法 SM-2 中的假设有本质上的不同。</p><p data-pid="f6Vsk25n"><b>总结：</b></p><ul><li data-pid="CBZLKgRS">sm17 (2016): SInc=f(C,S,R)， 3 个变量，f 是有适应性的</li><li data-pid="eXJttA71">sm5 (1989): SInc=f(C,S,0.9)， 2 个变量，f 是有适应性的</li><li data-pid="u6U1SBKo">sm2 (1987): SInc=f(SInc,S,0.9) - 1 个变量，f 是固定的</li></ul><hr/><p data-pid="aYKJ8205">从这篇开始，SuperMemo 的算法会越来越精致，也越来越硬核，所以不会翻译的太详细，想了解的朋友可以把我的翻译当作参考，具体细节还请看英文原文。</p><p data-pid="GOBH-08H">下一章：<a class="internal" href="./429504395.html">07 1990：记忆的通用公式</a></p>

<hr>
<div class="column" style="margin: 1em 0; padding: 0.5em 1em; border: 2px solid #999; border-radius: 5px;">
<h2>专栏：学校教育问题</h2>
</div>
<hr>
<p><a href="./">← 返回目录</a></p>
</article>
<script src="https://giscus.app/client.js"
data-repo="L-M-Sherlock/ZhiHuArchive"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNDk5NDE0MzM="
data-category="Announcements"
data-category-id="DIC_kwDOFNuuuc4Ck92x"
data-mapping="title"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="0"
data-input-position="top"
data-theme="preferred_color_scheme"
data-lang="zh-CN"
data-loading="lazy"
crossorigin="anonymous"
async>
</script>
</body>
</html>