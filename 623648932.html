<!DOCTYPE html>
<html lang="zh">
<head>
<title>rwkv.cpp: CPU 也能跑的 RNN 中文语言大模型 | ZhiHu Archive</title>
<meta charset="UTF-8">
<meta property="og:type" content="website">
<meta property="og:title" content="rwkv.cpp: CPU 也能跑的 RNN 中文语言大模型 | ZhiHu Archive">
<meta property="og:site_name" content="ZhiHu Archive for Thoughts Memo">
<meta property="og:url" content="https://zhuanlan.zhihu.com/p/623648932">
<meta property="og:image" content="https://picx.zhimg.com/v2-b1741c286684458848b12a83b289a003_720w.jpg?source=172ae18b">
<meta property="og:description" content="最近 LLM（大语言模型）实在火爆，出了不少开源模型，比如 Alpaca [1]、ChatGLM[2]、BELLE[3] 等等，让每个人都有机会运行和训练专属自己的 LLM，我也迫不及待了。但是，熟悉我的老读者朋友应该知道，虽然我是搞算法的，也发过几篇论文，但我是走的是贫穷科…">
<meta name="description" content="最近 LLM（大语言模型）实在火爆，出了不少开源模型，比如 Alpaca [1]、ChatGLM[2]、BELLE[3] 等等，让每个人都有机会运行和训练专属自己的 LLM，我也迫不及待了。但是，熟悉我的老读者朋友应该知道，虽然我是搞算法的，也发过几篇论文，但我是走的是贫穷科…">
<meta data-pagefind-meta="title" content="rwkv.cpp: CPU 也能跑的 RNN 中文语言大模型">
<meta data-pagefind-meta="image" content="https://picx.zhimg.com/v2-b1741c286684458848b12a83b289a003_720w.jpg?source=172ae18b">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:title" content="rwkv.cpp: CPU 也能跑的 RNN 中文语言大模型 | ZhiHu Archive">
<meta name="twitter:description" content="最近 LLM（大语言模型）实在火爆，出了不少开源模型，比如 Alpaca [1]、ChatGLM[2]、BELLE[3] 等等，让每个人都有机会运行和训练专属自己的 LLM，我也迫不及待了。但是，熟悉我的老读者朋友应该知道，虽然我是搞算法的，也发过几篇论文，但我是走的是贫穷科…">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0, user-scalable=no">
<meta name="google-site-verification" content="U7ZAFUgGNK60mmMqaRygg5vy-k8pwbPbDFXNjDCu7Xk" />
<link rel="alternate" type="application/rss+xml" title="ZhiHu Archive for Thoughts Memo" href="https://l-m-sherlock.github.io/ZhiHuArchive/feed.xml">
<link rel="stylesheet" href="https://gcore.jsdelivr.net/npm/yue.css@0.4.0/yue.css">
<script>
const redirect = false;
if (redirect) {
window.location.replace("https://zhuanlan.zhihu.com/p/623648932");
}
</script>
<style>
.origin_image {
width: 100%;
}
figure {
margin: 1.4em 0;
}
figure img {
width: 100%;
}
img {
vertical-align: middle;
}
.author {
display: flex;
gap: 1em;
align-items: center;
}
#avatar {
width: 100px;
height: 100px;
}
.author > div {
flex: 1;
}
a {
color: #2563eb;
text-decoration: none;
border-bottom: 1px solid rgba(37, 99, 235, 0.3);
border-radius: 4px;
padding: 0 0.1em;
transition: color 0.2s ease, border-color 0.2s ease, background-color 0.2s ease;
}
a:hover,
a:focus-visible {
color: #1d4ed8;
border-bottom-color: rgba(29, 78, 216, 0.6);
background-color: rgba(37, 99, 235, 0.08);
}
a:focus-visible {
outline: none;
box-shadow: 0 0 0 2px rgba(37, 99, 235, 0.25);
}
a[data-draft-type="link-card"] {
display: block;
border-bottom: none;
padding: 0;
background: none;
}
.references {
font-size: 0.85em;
}
.formula-display {
display: block;
text-align: center;
}
</style>
</head>
<body style="max-width: 1000px; margin: 0 auto; padding: 0 1em 0 1em;" class="yue">
<p data-pagefind-ignore><a href="./">← 返回目录</a></p>
<hr>
<header>
<img class="origin_image" src="https://picx.zhimg.com/v2-b1741c286684458848b12a83b289a003_720w.jpg?source=172ae18b"/>
<h1><a href="https://zhuanlan.zhihu.com/p/623648932" target="_blank" rel="noopener noreferrer">rwkv.cpp: CPU 也能跑的 RNN 中文语言大模型</a></h1>
<div class="author">
<img class="avatar" id="avatar" src="https://pica.zhimg.com/v2-d571786b77078321f5f6ef92f4967877_l.jpg?source=172ae18b" />
<div>
<h2 rel="author">
<a href="https://www.zhihu.com/people/4c592f496dc33822b560b382907ff1d0" target="_blank" rel="noopener noreferrer">@Thoughts Memo</a>
</h2>
<p>学校≠教育≠技能；文凭溢价=80%信号传递+20%人力资本</p>
</div>
</div>
<time datetime="2023-04-21T04:14:23">发表于 2023年04月21日</time>
<p rel="stats"style="color: #999; font-size: 0.9em;">185 👍 / 59 💬</p>
</header>
<article data-pagefind-body>
<p data-pid="zV92JVVP">最近 LLM（大语言模型）实在火爆，出了不少开源模型，比如 Alpaca<sup data-draft-node="inline" data-draft-type="reference" data-numero="1" data-text="" data-url="https://github.com/antimatter15/alpaca.cpp">[1]</sup>、ChatGLM<sup data-draft-node="inline" data-draft-type="reference" data-numero="2" data-text="" data-url="https://github.com/THUDM/ChatGLM-6B">[2]</sup>、BELLE<sup data-draft-node="inline" data-draft-type="reference" data-numero="3" data-text="" data-url="https://github.com/LianjiaTech/BELLE">[3]</sup> 等等，让每个人都有机会运行和训练专属自己的 LLM，我也迫不及待了。</p><p data-pid="cK40rdEd">但是，熟悉我的老读者朋友应该知道，虽然我是搞算法的，也发过几篇论文，但我是走的是贫穷科研的路线，一张显卡都没有。像 ChatGLM-6B 这种模型，在我的小破 Mac 上根本跑不起来。Alpaca 的 CPU 版本虽然能跑，但它中文水平实在太烂了。有没有什么模型不仅中文好，又可以不依赖显卡？RWKV<sup data-draft-node="inline" data-draft-type="reference" data-numero="4" data-text="发布几个RWKV的Chat模型（包括英文和中文）7B/14B欢迎大家玩 - PENG Bo的文章 - 知乎 " data-url="https://zhuanlan.zhihu.com/p/618011122">[4]</sup>进入了我的视野。</p><p data-pid="VP86X0E_">RWKV 是一种纯 RNN 的架构，能够进行语言建模<sup data-draft-node="inline" data-draft-type="reference" data-numero="5" data-text="RWKV-v2-RNN 原理：超越 Transformer，实现 O(T) 的语言建模 - PENG Bo的文章 - 知乎 " data-url="https://zhuanlan.zhihu.com/p/514840332">[5]</sup>，目前最大参数规模已经做到了 14B<sup data-draft-node="inline" data-draft-type="reference" data-numero="6" data-text="RWKV：用RNN达到Transformer性能，且支持并行模式和长程记忆，既快又省显存，已在14B参数规模检验 - PENG Bo的文章 - 知乎 " data-url="https://zhuanlan.zhihu.com/p/599150009">[6]</sup>。目前的在线体验地址：</p><a class="wrap external" data-draft-node="block" data-draft-type="link-card" data-image="https://pica.zhimg.com/v2-638b13335ae7509e6719410f2b3c0d60_qhd.jpg" data-image-height="648" data-image-width="1200" href="https://huggingface.co/spaces/BlinkDL/Raven-RWKV-7B" rel="nofollow noreferrer noopener" target="_blank">Raven RWKV 7B - a Hugging Face Space by BlinkDL</a><p data-pid="X2bRRBAb">不过请注意，上面这个体验模型的微调语料英文占 99%，所以中文水平并不是最好的。作者 <a class="member_mention" data-hash="64d972cc5bae62489442e35b32dc0fce" data-hovercard="p$b$64d972cc5bae62489442e35b32dc0fce" href="https://www.zhihu.com/people/64d972cc5bae62489442e35b32dc0fce" rel="noopener noreferrer" target="_blank">@PENG Bo</a> 最近发布的 RWKV-4-Raven-7B-v9x-Eng49%-Chn50%-Other1% 这个模型的中文微调语料占 50%，中文水平更好。以下我也会基于该模型进行操作。</p><h2>下载模型</h2><p data-pid="Lk_3zpC2">首先，RWKV 的模型分为很多种，都发布在作者的 huggingface<sup data-draft-node="inline" data-draft-type="reference" data-numero="7" data-text="BlinkDL (BlinkDL) (huggingface.co)" data-url="https://huggingface.co/BlinkDL">[7]</sup> 上：</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pica.zhimg.com/v2-8881c00c2d9940545e76e28c29794f96_r.jpg" data-original-token="v2-8881c00c2d9940545e76e28c29794f96" data-rawheight="678" data-rawwidth="1164" data-size="normal" src="https://pica.zhimg.com/v2-8881c00c2d9940545e76e28c29794f96_1440w.jpg" width="1164"/></figure><p data-pid="qyq-8GoQ">其中：</p><ul><li data-pid="29nztvWv">统一前缀 <code>rwkv-4</code> 表示它们都基于 RWKV 的第 4 代架构。 </li><li data-pid="aVW9XWzD">pile 代表基底模型，在 pile 等基础语料上进行预训练，没有进行微调，适合高玩来给自己定制。</li><li data-pid="x8Ng4zFM">novel 代表小说模型，在各种语言的小说上进行微调，适合写小说。</li><li data-pid="DAF8BwRB">raven 代表对话模型，在各种开源的对话语料上进行微调，适合聊天、问答、写代码。</li><li data-pid="NmTwccTf">430m、7b 这些指的是模型的参数量。</li></ul><p data-pid="3_znQPzM">我下载的是 <code>RWKV-4-Raven-7B-v9x-Eng49%-Chn50%-Other1%-20230418-ctx4096.pth</code><sup data-draft-node="inline" data-draft-type="reference" data-numero="8" data-text=" RWKV-4-Raven-7B-v9x-Eng49%-Chn50%-Other1%-20230418-ctx4096.pth" data-url="https://huggingface.co/BlinkDL/rwkv-4-raven/blob/main/RWKV-4-Raven-7B-v9x-Eng49%25-Chn50%25-Other1%25-20230418-ctx4096.pth">[8]</sup> ， 即参数量为 7B 的对话模型，微调预料中 49% 是英文，50% 是中文。ctx4096 表示微调是的上下文长度。</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pic3.zhimg.com/v2-1227b07bc19529ebed99f6af17d81566_r.jpg" data-original-token="v2-1227b07bc19529ebed99f6af17d81566" data-rawheight="638" data-rawwidth="1096" data-size="normal" src="https://pic3.zhimg.com/v2-1227b07bc19529ebed99f6af17d81566_1440w.jpg" width="1096"/></figure><p data-pid="tF8Das4o">这个模型有 14.8 GB，请务必确保自己电脑的可用硬盘空间在 40 GB 以上（因为后面要对这个模型进行转换和量化，需要占用更多的硬盘空间。）</p><h2>模型转换</h2><p data-pid="w7gj6MLG">下载好的模型配合 ChatRWKV<sup data-draft-node="inline" data-draft-type="reference" data-numero="9" data-text="" data-url="https://github.com/BlinkDL/ChatRWKV">[9]</sup>这个仓库里的代码就可以跑了，但是它对 CPU 策略的支持最低只到 fp32i8，7B 模型需要 12 GB 内存才能跑起来，我用 16GB 内存的 Mac 试了一下，跑是跑起来了，但是非常慢。</p><p data-pid="zml5N4fi">所以，这里需要介绍一下能够更充分利用 CPU 的方法：</p><a class="wrap external" data-draft-node="block" data-draft-type="link-card" href="https://github.com/saharNooby/rwkv.cpp" rel="nofollow noreferrer noopener" target="_blank">saharNooby/rwkv.cpp: INT4 and FP16 inference on CPU for RWKV language model (github.com)</a><p data-pid="QJSmsUe4">rwkv.cpp 可以将 RWKV 原始模型的参数转化为 float16，并量化到 int4，可以在 CPU 上更快地运行，也可以节省更多的内存。</p><p data-pid="vrrJ7ADM">以下是操作步骤。</p><h3>1. 下载仓库代码</h3><p data-pid="AnXwWbBn">需要安装 git（没有 git 的朋友，可能你要补的前置知识有点多，我建议自行搜索）</p><div class="highlight"><pre><code class="language-console">git clone --recursive https://github.com/saharNooby/rwkv.cpp.git
cd rwkv.cpp</code></pre></div><h3>2. 下载依赖库 or 编译依赖库</h3><p data-pid="c2oyEQSU">rwkv.cpp 的开发者已经预编译了不同平台上的依赖库，可以在这里下载：<a class="external" href="https://github.com/saharNooby/rwkv.cpp/releases" rel="nofollow noreferrer noopener" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/saharNooby/r</span><span class="invisible">wkv.cpp/releases</span><span class="ellipsis"></span></a></p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://picx.zhimg.com/v2-0e4e1e401f478835a3d093653a594751_r.jpg" data-original-token="v2-0e4e1e401f478835a3d093653a594751" data-rawheight="608" data-rawwidth="1858" data-size="normal" src="https://picx.zhimg.com/v2-0e4e1e401f478835a3d093653a594751_1440w.jpg" width="1858"/></figure><p data-pid="5KJnib-y">下载的时候请注意操作系统类型和支持的架构。由于作者没有预编译对 Mac m1 的 ARM64 架构的依赖库，所以我选择自行编译（需要安装 cmake，并在 shell 中移动到 rwkv.cpp 路径下）：</p><div class="highlight"><pre><code class="language-bash">cmake -DBUILD_SHARED_LIBS<span class="o">=</span>ON .
cmake --build . --config Release</code></pre></div><h3>3. 转换模型</h3><p data-pid="jiPvPLpz">需要装 PyTorch</p><p data-pid="tWZmsGAe">我直接把下载好的模型放在了 rwkv.cpp 的路径下，然后执行以下命令：</p><div class="highlight"><pre><code class="language-text">python rwkv/convert_pytorch_to_ggml.py ./RWKV-4-Raven-7B-v9x-Eng49%-Chn50%-Other1%-20230418-ctx4096.pth ./rwkv.cpp-7B.bin float16</code></pre></div><p data-pid="tWiX-UuH">其实就是让 python 运行 <code>rwkv/convert_pytorch_to_ggml.py</code> 这个转换模型的代码， <code>./RWKV-4-Raven-7B-v9x-Eng49%-Chn50%-Other1%-20230418-ctx4096.pth</code> 是待转换的模型的路径， <code>./rwkv.cpp-7B.bin</code> 是转换后的路径，float16 是要转换成什么类型的参数。</p><h3>4. 量化模型</h3><p data-pid="QYyd-Par">其实上面转换好的 <code>./rwkv.cpp-7B.bin</code> 已经可以用了，但是它要占用 16GB 内存。为了减少内存占用，可以将 <code>./rwkv.cpp-7B.bin</code> 量化为 int4，可以省一半内存，也就是只占 6GB 内存。只需要执行以下命令：</p><div class="highlight"><pre><code class="language-text">python rwkv/quantize.py ./rwkv.cpp-7B.bin ./rwkv.cpp-7B-Q4_1_O.bin 4</code></pre></div><p data-pid="OTwusGBk">然后你就会得到一个大小只有 6GB 的模型了。</p><h2>运行模型</h2><p data-pid="69Zh7h4b">同样，一行命令搞定：</p><div class="highlight"><pre><code class="language-bash">python rwkv/chat_with_bot.py ./rwkv.cpp-7B-Q4_1_0.bin</code></pre></div><p data-pid="JEFApSQQ">让我们看看效果，首先是内存占用，不到 6GB</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pica.zhimg.com/v2-4d614aad71b16c9588d241f07bdd72c0_r.jpg" data-original-token="v2-4d614aad71b16c9588d241f07bdd72c0" data-rawheight="532" data-rawwidth="833" data-size="normal" src="https://pica.zhimg.com/v2-4d614aad71b16c9588d241f07bdd72c0_1440w.jpg" width="833"/></figure><p data-pid="833CUEH-">然后是问答和执行命令：</p><ol><li data-pid="ayB-TwXl">太阳有几只眼睛？</li><li data-pid="fFAj0O6u">知乎是什么网站？</li><li data-pid="Mz_c-c5j">写一篇介绍数据分析的文章。</li></ol><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pic3.zhimg.com/v2-486e3098f940511231b353ff5ca2783c_r.jpg" data-original-token="v2-486e3098f940511231b353ff5ca2783c" data-rawheight="412" data-rawwidth="833" data-size="normal" src="https://pic3.zhimg.com/v2-486e3098f940511231b353ff5ca2783c_1440w.jpg" width="833"/></figure><p data-pid="fYjcvbC-">效果还不错，不过最后莫名其妙又多说了一段话，可能是量化带来的精度损失？非量化版本的效果如下：</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pica.zhimg.com/v2-673e9a0cde7e3f50fe20994a5d0eec72_r.jpg" data-original-token="v2-673e9a0cde7e3f50fe20994a5d0eec72" data-rawheight="532" data-rawwidth="785" data-size="normal" src="https://pica.zhimg.com/v2-673e9a0cde7e3f50fe20994a5d0eec72_1440w.jpg" width="785"/></figure><p data-pid="WXSL4fSR">希望开发者之后能对量化版本进行测评，让 rwkv 变得更好用。</p><p data-pid="jPMKNGxm">更新：补一张 GIF</p><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pic2.zhimg.com/v2-91e647db7609d28968d2e234bd661df5_r.jpg" data-original-token="v2-91e647db7609d28968d2e234bd661df5" data-rawheight="1056" data-rawwidth="1660" data-size="normal" src="https://pic2.zhimg.com/v2-91e647db7609d28968d2e234bd661df5_1440w.jpg" width="1660"/></figure><hr/><p data-pid="V4RkBS63">以上就是我在 Mac 上用 6GB 内存运行 7B 的中文语言模型 RWKV 的过程了，希望对读者朋友们有所帮助。</p><p data-pid="y4rty5Pz">另外也偷偷推一下自己的项目，使用时序模型来预测学习者的记忆状态，提高间隔重复中的复习效率，目前已经 600+ star：</p><a class="wrap external" data-draft-node="block" data-draft-type="link-card" href="https://github.com/open-spaced-repetition/fsrs4anki" rel="nofollow noreferrer noopener" target="_blank">open-spaced-repetition/fsrs4anki: A modern Anki custom scheduling based on free spaced repetition scheduler algorithm (github.com)</a><figure data-size="normal"><img class="origin_image zh-lightbox-thumb" data-caption="" data-original="https://pic4.zhimg.com/v2-2fbb3c26d57b2ae769d994a6ac6a60ef_r.jpg" data-original-token="v2-2fbb3c26d57b2ae769d994a6ac6a60ef" data-rawheight="533" data-rawwidth="800" data-size="normal" src="https://pic4.zhimg.com/v2-2fbb3c26d57b2ae769d994a6ac6a60ef_1440w.jpg" width="800"/></figure><p data-pid="1Y0cUa-V">相关文章：</p><a class="internal" data-draft-node="block" data-draft-type="link-card" data-image="https://pic1.zhimg.com/v2-08134e67de67b70a126b02829d90edcc_qhd.jpg" data-image-height="1592" data-image-width="2251" href="./543325359.html" rel="noopener noreferrer" target="_blank">叶峻峣：我是如何在本科期间发表顶会论文的？（内含开源代码和数据集）</a><a class="internal" data-draft-node="block" data-draft-type="link-card" data-image="https://pic2.zhimg.com/v2-337e676d732039a25471c1057c54bfa3_qhd.jpg" data-image-height="1480" data-image-width="2700" href="./577383961.html" rel="noopener noreferrer" target="_blank">叶峻峣：KDD'22 | 墨墨背单词：基于时序模型与最优控制的记忆算法 [AI+教育]</a><a class="internal" data-draft-node="block" data-draft-type="link-card" data-image="https://pica.zhimg.com/v2-c8cb226c91641848424b5edf5e6057b8_l.jpg" data-image-height="1912" data-image-width="1931" href="./591833332.html" rel="noopener noreferrer" target="_blank">叶峻峣：如何在 Anki 上使用次世代间隔重复算法 FSRS？</a><p></p>
<hr><section data-pagefind-ignore><h2>参考</h2>4. 发布几个RWKV的Chat模型（包括英文和中文）7B/14B欢迎大家玩 - PENG Bo的文章 - 知乎  <a href="https://zhuanlan.zhihu.com/p/618011122" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/618011122</a><br>5. RWKV-v2-RNN 原理：超越 Transformer，实现 O(T) 的语言建模 - PENG Bo的文章 - 知乎  <a href="https://zhuanlan.zhihu.com/p/514840332" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/514840332</a><br>6. RWKV：用RNN达到Transformer性能，且支持并行模式和长程记忆，既快又省显存，已在14B参数规模检验 - PENG Bo的文章 - 知乎  <a href="https://zhuanlan.zhihu.com/p/599150009" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/599150009</a><br>7. BlinkDL (BlinkDL) (huggingface.co) <a href="https://huggingface.co/BlinkDL" target="_blank" rel="noopener noreferrer">https://huggingface.co/BlinkDL</a><br>8.  RWKV-4-Raven-7B-v9x-Eng49%-Chn50%-Other1%-20230418-ctx4096.pth <a href="https://huggingface.co/BlinkDL/rwkv-4-raven/blob/main/RWKV-4-Raven-7B-v9x-Eng49%25-Chn50%25-Other1%25-20230418-ctx4096.pth" target="_blank" rel="noopener noreferrer">https://huggingface.co/BlinkDL/rwkv-4-raven/blob/main/RWKV-4-Raven-7B-v9x-Eng49%25-Chn50%25-Other1%25-20230418-ctx4096.pth</a></section>
<hr>
<div class="column" style="margin: 1em 0; padding: 0.5em 1em; border: 2px solid #999; border-radius: 5px;" data-pagefind-ignore>
<h2>专栏：学委叶哥的随笔</h2>
<p></p>
</div>
<hr>
<p data-pagefind-ignore><a href="./">← 返回目录</a></p>
</article>
<footer>
<p style="color: #999; font-size: 0.85em; text-align: center; margin-top: 2em;">
本页面由 <a href="https://github.com/L-M-Sherlock/ZhiHuArchive" target="_blank" rel="noopener noreferrer">ZhiHuArchive</a> 渲染，模板参考 <a href="https://github.com/frostming/fxzhihu" target="_blank" rel="noopener noreferrer">FxZhihu</a>。
</p>
</footer>
<script src="https://giscus.app/client.js"
data-repo="L-M-Sherlock/ZhiHuArchive"
data-repo-id="MDEwOlJlcG9zaXRvcnkzNDk5NDE0MzM="
data-category="Announcements"
data-category-id="DIC_kwDOFNuuuc4Ck92x"
data-mapping="title"
data-strict="0"
data-reactions-enabled="1"
data-emit-metadata="0"
data-input-position="top"
data-theme="preferred_color_scheme"
data-lang="zh-CN"
data-loading="lazy"
crossorigin="anonymous"
async>
</script>
</body>
</html>