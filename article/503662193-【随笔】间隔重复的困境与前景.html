<meta charset="UTF-8">
【随笔】间隔重复的困境与前景
<div><blockquote>题图：<a class="wrap external" href="https://www.pixiv.net/artworks/96122959" rel="nofollow noreferrer" target="_blank">#原神 snowman - TOMOGY的插画 - pixiv</a></blockquote><p>离上一次写原创，又过去一个多月了。汉化组又翻译了不少文章，而我也在推进自己的研究进展，同时思考了间隔重复面临的困境与前景。这次就和大家分享一下我目前的进展和思考。</p><h2><b>研究进展</b></h2><figure><img class="origin_image zh-lightbox-thumb lazy" data-actualsrc="https://pic1.zhimg.com/v2-9dd43da2e7d087e927b6045d3ac0468c_b.jpg" data-caption="" src="https://pic1.zhimg.com/v2-9dd43da2e7d087e927b6045d3ac0468c_r.jpg" data-rawheight="400" data-rawwidth="2874" data-size="normal" src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='2874' height='400'&gt;&lt;/svg&gt;" width="2874"/></figure><figure><img class="origin_image zh-lightbox-thumb lazy" data-actualsrc="https://pic3.zhimg.com/v2-4c626d6603df5ec1373824fb434f6f82_b.jpg" data-caption="" src="https://pic3.zhimg.com/v2-4c626d6603df5ec1373824fb434f6f82_r.jpg" data-rawheight="312" data-rawwidth="1859" data-size="normal" src="data:image/svg+xml;utf8,&lt;svg xmlns='http://www.w3.org/2000/svg' width='1859' height='312'&gt;&lt;/svg&gt;" width="1859"/></figure><p>去年 10 月份，我投出了人生中的第一份论文，并被中文信息学报录用，这篇论文是关于长期记忆预测的。然后是今年 2 月份，我投出了第一篇英文论文，还在 SIGKDD 的审稿周期中，这篇论文是关于间隔重复调度优化的。目前我正在写我的第三篇论文，算是对前两份工作的整合，这也是未来一个月我会闭关的原因之一。</p><p>其实我对在知乎上分享我的研究进展很有兴趣，但由于论文一直没有被刊出，如果分享工作的细节，可能会影响发表。这一点上我比较谨慎，希望这篇论文能早日刊出，这样我也能松一口气了。</p><p>目前我的研究进展已经推进到基于大规模数据的间隔重复算法实现上了，但由于模型较为复杂，还无法直接被用于个人的间隔重复系统，未来的研究会朝这个方向努力。</p><p>更多的研究进展，我会在论文发表后与大家分享，还请见谅。不过本期对间隔重复困境的思考中会包含算法相关的问题，感兴趣的朋友可以继续往下看。</p><h2><b>困境</b></h2><p>任何一种学习方法，都有其局限性，间隔重复也不例外。而若想要间隔重复这一方法能够帮助到你，有以下问题需要回答：</p><ol><li>间隔重复能让你建立持久的长期记忆吗？</li><li>间隔重复的时间成本可以被你接受吗？</li><li>如果你不用间隔重复，是不是就会忘记？</li><li>你通过间隔重复记住的知识能实际应用吗？</li></ol><p>从目前的研究来看，这些问题的回答如下：</p><p>对于 1 的回答是：可以建立持久的长期记忆，但前提条件是知识的合理表述（<a class="internal" href="http://zhuanlan.zhihu.com/p/434074509">如何写出好卡片</a>）。表述非常糟糕的卡片往往会变成<a class="internal" href="http://zhuanlan.zhihu.com/p/307602868">水蛭（Leech）[Anki, SuperMemo]</a>，乃至<a class="internal" href="http://zhuanlan.zhihu.com/p/67390960">毒性记忆</a>。而合理的知识表述结合间隔重复，能够形成保持数月、数年，乃至数十年的长期记忆。</p><p>对于 2 的回答是：这其实包含了一个价值问题和一个事实问题。</p><ol><li>价值问题：记忆的内容是否值得我们付出时间？</li><li>事实问题：间隔重复需要的时间是否合理？</li></ol><p>对于 2.1 的回答是：只间隔重复那些你认为重要的内容就好了。</p><p>对于 2.2 的回答是：间隔重复需要的时间和你的知识表述有关。一张表述良好的卡片，使用间隔重复，一辈子只需要花 1 分钟。但表述糟糕的卡片，可能需要十倍（甚至更多）于此的时间。</p><p>对于 3 的回答是：对于天天都用的知识而言，确实很难忘记。但在创造性的工作中，我们需要的知识往往来自多个学科，并且包含大量细节。如果我们不间隔重复，我们真的就会忘记这些知识。</p><p>对于 4 的回答是：就我个人经验而言，是可以的，我平时写论文和做实验设计，大量依赖于我在间隔重复中记住的相关知识。当然个人经验的说服力并不大，感兴趣的朋友可以看这篇论文：<a class="wrap external" href="https://andymatuschak.org/files/Butler%2520-%25202010%2520-%2520Repeated%2520Testing%2520Produces%2520Superior%2520Transfer%2520of%2520Learning%2520Relative%2520to%2520Repeated.pdf" rel="nofollow noreferrer" target="_blank">Butler - 2010 - Repeated Testing Produces Superior Transfer of Learning Relative to Repeated.pdf (andymatuschak.org)</a>。但这些知识的迁移性也受到知识表述的限制。</p><p>看起来这些问题都能回答，那么困境究竟在哪里呢？</p><h3><b>知识的合理表述</b></h3><p>简简单单的 7 个字，难度却不小。如何让学习者编写出容易记忆的卡片，一直是推广间隔重复的难点之一。</p><p>从上面这些回答中，我们发现，知识的合理表述对 1、2、4 都有影响。知识表述太差，（1）即使间隔重复了也记不住；（2）即使记住也要花大量时间；（3）即使花了时间记住，也应用不到实际中。</p><p>支持（1）的证据有：<a class="internal" href="https://zhuanlan.zhihu.com/p/471760789#h_471760789_18">复合记忆痕迹的子稳定性收敛情况</a></p><p>支持（2）的证据来自我的算法模拟：<a class="wrap external" href="https://github.com/maimemo/SSP-MMC/tree/main/algo/result" rel="nofollow noreferrer" target="_blank">Maimemo/SSP-MMC/algo/result</a></p><p>支持（3）的证据有：<a class="internal" href="https://zhuanlan.zhihu.com/p/401944262">编写间隔重复记忆卡片应避免浅层次的「模式匹配」</a></p><p>即使最终学习者能够写出知识表述良好的卡片，但在制卡上花费太多时间，最终也会影响问题 2 的回答。</p><h3><b>间隔重复的算法</b></h3><p>抛开知识的合理表述不谈，间隔重复的算法也很重要，但它的重要性经常被人质疑。</p><p>其实用不同的算法对间隔重复过程进行模拟，比较一下同样记忆 1000 张卡片，各需要多少时间，最终记住了多少，自然就能比较出不同算法之间的效率差距。在我自己的论文研究中，这种差距可以达到 3 倍。也就是说，用两种算法，在付出同样的时间成本下，好算法可以记住的卡片数量是差算法的 3 倍。</p><p>当然，模拟终究是模拟，但这里也有真实使用实验的证据：</p><blockquote>In the second half of 2019, we made a number of improvements to the platform, which have cumulatively resulted in compressing this curve by more than two repetitions. That is, readers now achieve a noticeably higher degree of demonstrated retention after four repetitions than readers did last year after six.<br/><br/>——<a class="wrap external" href="https://www.patreon.com/posts/skip-exponential-40672377" rel="nofollow noreferrer" target="_blank">“Skip”: exponential-backoff deferral mechanisms and fuzzy inboxes | Patreon 上的 Andy Matuschak</a></blockquote><p>通过改进算法，Andy 将重复次数从 6 次缩减到 4 次，并提高了保留率。</p><p>但间隔重复算法的困难在于如何克服数据稀疏，在学习者有限次复习内快速完成对记忆的建模，提供高效的复习安排。</p><p>而目前唯一能针对个人用户优化的间隔重复算法只有 SuperMemo 系列。 Andy、Duolingo、Maimemo 等间隔重复算法都是根据大量数据来优化间隔。</p><h2><b>前景</b></h2><p>关于知识的合理表述，已经有不同的解决路径——<a class="internal" href="https://zhuanlan.zhihu.com/p/396445859">间隔重复软件的两大进路</a>：</p><ul><li>一条是 SuperMemo、RemNote 等软件所采用的，通过辅助制卡来降低个人制卡成本；</li><li>另一条是 Andy 的助记媒介所探索的，通过提供专家卡片和上下文来降低人均制卡成本。</li></ul><blockquote>补充：还有一条路是使用机器学习算法生成卡片<sup><a aria-labelledby="ref_1" data-reference-link="true" href="#ref_1" id="ref_1_0">[1]</a></sup><sup><a aria-labelledby="ref_2" data-reference-link="true" href="#ref_2" id="ref_2_0">[2]</a></sup><sup><a aria-labelledby="ref_3" data-reference-link="true" href="#ref_3" id="ref_3_0">[3]</a></sup>，但离大规模应用还有很长的路要走。</blockquote><p>但这些解决路径进展并不顺利（至少在国内）：</p><ul><li>SuperMemo 门槛太高，哪怕国内的志愿者们制作了大量的教程和辅助工具，依然没有让 SM 达到开箱即用的程度。</li><li>RemNote 对中文支持不佳，更不用说它还有个别名叫 BugNote，还不足以让每个人都能流畅的使用。</li><li>Andy 的助记媒介，几乎没有国内的材料，并且他画的好几个饼都没填完。</li></ul><p>更不用说，大多数人入坑后都是在用 Anki，背着别人编写的卡片，然后在吃尽苦头后放弃。</p><p>看来间隔重复的前景并不明朗，但这些不顺利并不像 20 世纪之初物理学的「两朵乌云」那么致命。这正是我的机会。我目前除了天天使用 SuperMemo 外，一方面在进行间隔重复算法的研究，另一方面也正在使用助记媒介，从中获得灵感。</p><p>闭关之后我会在知乎上<a class="internal" href="https://zhuanlan.zhihu.com/p/457529308">开门造车</a>，和大家同步最新的进展。</p><p>好了，空话就不多说了，我闭关去了，后会有期。</p><p><br/></p><p>2022 年 4 月 23 日</p><p>叶峻峣</p><h2>参考</h2><ol><li><a aria-label="back" class="ReferenceList-backLink" data-reference-link="true" href="#ref_1_0">^</a><span>Improving Language Model Behavior by Training on a Curated Dataset</span> <a class="external" href="https://openai.com/blog/improving-language-model-behavior/" rel="noopener noreferrer" target="_blank">https://openai.com/blog/improving-language-model-behavior/</a></li><li><a aria-label="back" class="ReferenceList-backLink" data-reference-link="true" href="#ref_2_0">^</a><span>t5-base-question-generator</span> <a class="external" href="https://huggingface.co/iarfmoose/t5-base-question-generator" rel="noopener noreferrer" target="_blank">https://huggingface.co/iarfmoose/t5-base-question-generator</a></li><li><a aria-label="back" class="ReferenceList-backLink" data-reference-link="true" href="#ref_3_0">^</a><span>Automatic question-answer pairs generation and question similarity mechanism in question answering system</span> <a class="external" href="https://dl.acm.org/doi/abs/10.1007/s10489-021-02348-9" rel="noopener noreferrer" target="_blank">https://dl.acm.org/doi/abs/10.1007/s10489-021-02348-9</a></li></ol></div>