{
  "status": 0,
  "updated": 1610958314,
  "author": {
    "is_followed": false,
    "badge": [
      {
        "type": "identity",
        "description": "信息技术行业 算法工程师"
      }
    ],
    "name": "Thoughts Memo",
    "url": "https://api.zhihu.com/people/4c592f496dc33822b560b382907ff1d0",
    "gender": 1,
    "user_type": "people",
    "headline": "学校≠教育≠技能；文凭溢价=80%信号传递+20%人力资本",
    "avatar_url": "https://picx.zhimg.com/50/v2-41f893b0cd84fb6a8c8f6d1bd29c9554_l.jpg?source=b1748391",
    "is_following": false,
    "type": "people",
    "id": "4c592f496dc33822b560b382907ff1d0"
  },
  "can_tip": false,
  "excerpt": "看到上期评论区有读者想要了解一下 @多邻国Duolingo 的 HLR 算法[1]的细节，正好我…",
  "tipjarors_count": 0,
  "admin_closed_comment": false,
  "reason": "",
  "excerpt_title": "",
  "id": 345172257,
  "voteup_count": 28,
  "can_comment": {
    "status": true,
    "reason": ""
  },
  "created": 1610957735,
  "url": "https://api.zhihu.com/articles/345172257",
  "comment_permission": "all",
  "title": "从 Duolingo 机器学习算法说起，浅析记忆数据的特征工程",
  "image_width": 1181,
  "content": "<p data-pid=\"uOxv-Ewr\">看到上期评论区有读者想要了解一下 <a class=\"member_mention\" href=\"https://www.zhihu.com/people/95f1e522f9bd49af82256f8ff031d2f7\" data-hash=\"95f1e522f9bd49af82256f8ff031d2f7\" data-hovercard=\"p$b$95f1e522f9bd49af82256f8ff031d2f7\">@多邻国Duolingo</a> 的 HLR 算法<sup data-text=\"duolingo / halflife-regression\" data-url=\"https://github.com/duolingo/halflife-regression\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"1\">[1]</sup>的细节，正好我之前因为工作原因有整理过相关文档，本期就从 HLR 算法说起，讲讲机器学习在复习算法中的应用。</p><p data-pid=\"QJMFsivo\">注：以下与 HLR 算法相关的内容，均参考自 <a href=\"https://link.zhihu.com/?target=https%3A//research.duolingo.com/papers/settles.acl16.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A Trainable Spaced Repetition Model for Language Learning B. Settles and B. Meeder ACL Proceedings, 2016</a></p><h2>算法简介</h2><p data-pid=\"GQfCtP7H\">HLR 是 Half-life Regression 的缩写，直译为中文就是半衰期回归。半衰期在这里的含义是遗忘概率达到 50% 时的复习间隔（类似记忆稳定性<sup data-text=\"从一个记忆伪概念，到记忆研究的难题。\" data-url=\"https://zhuanlan.zhihu.com/p/343115387\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"2\">[2]</sup>）。而回归指的是该算法应用了回归模型，用以拟合记忆的半衰期。</p><p data-pid=\"Wm5XWdnF\">这样讲有点抽象，不过 HLR 算法作为机器学习算法的代表，可以直接从机器学习的角度来理解。对于一个机器学习任务，最重要的两个部分就是数据和模型。让我们看看 HLR 到底用了怎样的数据和模型吧！</p><h2>数据</h2><p data-pid=\"0zSbKBFT\">以下是 HLR 算法的数据样例</p><h3>样本示例</h3><figure data-size=\"normal\"><img src=\"https://pica.zhimg.com/v2-6f670e16ef98e6a87218f16252f6c802_b.jpg\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb\" width=\"1370\" data-original=\"https://pica.zhimg.com/v2-6f670e16ef98e6a87218f16252f6c802_r.jpg\" data-original-token=\"v2-80727a3efbf9c7143ecb12a8270edccd\"/><figcaption>本来是想贴表格的，但是知乎最大列数只有 8</figcaption></figure><h3>属性注解</h3><ul><li data-pid=\"yx2_60Hg\"> p_recall：回忆概率，用一轮练习中一个单词被正确回忆的次数比去总共回忆的次数来表示；</li><li data-pid=\"KwHXB39R\"> timestamp：练习的日期</li><li data-pid=\"I0Ts9GFx\"> delta：距离上次练习该单词的时间</li><li data-pid=\"Ywt9Mh03\"> user_id：学生 id</li><li data-pid=\"K7qxpKJi\"> learning_language：学习的语言</li><li data-pid=\"pDFHkVrD\"> ui_language：用户 ui 语言</li><li data-pid=\"RDYbDw6W\"> lexeme_id：词位 id</li><li data-pid=\"681fkJT_\"> lexeme_string：词位标签</li><li data-pid=\"qFQrRGSV\"> session_seen：本次练习该单词次数</li><li data-pid=\"k7GIF1G1\"> session_correct：本次练习正确次数</li></ul><p data-pid=\"KfGdxCWx\">从这个数据中我们就能看出一些有用的信息。比如，学生 id 表示了学习者，词位 id 表示了记忆材料，这两者在直觉上似乎就与记忆有关。还有练习次数、回忆正确次数等等，相当于记忆行为的历史特征，表示在过去学习者对材料的反馈情况。当然，可千万别忘记了一个重要的特征，那就是距离上次练习该单词的时间。</p><h2>模型</h2><p data-pid=\"bxeLfAmT\">看完了数据，就该好好思考一下如何用一个模型来利用这些数据了。别忘了复习算法的目标，那就是给出高效的复习安排，而实现这一目标的基础，正是对记忆本身的精确预测。</p><p data-pid=\"6bK_E7sw\">想要预测，就要建模。而建模，就少不了输入输出，也就是特征向量（feature vector）和标记（label）。</p><h3>特征向量</h3><p data-pid=\"mAi4owlU\">HLR 的特征向量是 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+x+%3D+%28right%2Cwrong%2Cbias%2C%5Cvec%7Blex%7D%29\" alt=\"\\vec x = (right,wrong,bias,\\vec{lex})\" eeimg=\"1\"/> ，其中：</p><ul><li data-pid=\"wHVLk1el\">right： <img src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7B1%2B%5Ctext%7Bhistory_correct%7D%7D\" alt=\"\\sqrt{1+\\text{history_correct}}\" eeimg=\"1\"/> </li><li data-pid=\"rlLOYNJ6\">wrong： <img src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7B1%2B%28%5Ctext%7Bhistory_seen%7D-%5Ctext%7Bhistory_correct%7D%29%7D\" alt=\"\\sqrt{1+(\\text{history_seen}-\\text{history_correct})}\" eeimg=\"1\"/> </li><li data-pid=\"rvhQFiuj\"><img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Blex%7D\" alt=\"\\vec{lex}\" eeimg=\"1\"/> ：20k 维的稀疏向量，对应哪个词位，其值设为 1，其余为 0</li><li data-pid=\"PSATlpdU\">bias：1</li></ul><p data-pid=\"ehzU8dCL\">可以很明显地看出，HLR 将记忆反馈历史转化为历史累计答对次数（right）和答错次数（wrong），还用到了记忆材料本身（lex）。</p><h3>标记</h3><p data-pid=\"QAAc2Swl\">HLR 的标记是回忆概率（p_recall），用来表示学习者实际的反馈，即同一个单词的相关测试正确率。</p><h3>预测函数</h3><p data-pid=\"7WfK_G7-\">HLR 的核心就是预测函数，这是特征与标记之间的桥梁。不同于常见的线性模型，HLR 在特征与标记之间加入了名为半衰期的中间变量。最终的预测函数为：</p><ul><li data-pid=\"7t4lX0NI\"><img src=\"https://www.zhihu.com/equation?tex=%5Chat+h%3D2%5E%7B%5Cvec%5Ctheta%5Ccdot%5Cvec+x%7D\" alt=\"\\hat h=2^{\\vec\\theta\\cdot\\vec x}\" eeimg=\"1\"/> ：半衰期</li><li data-pid=\"23GljKrx\"><img src=\"https://www.zhihu.com/equation?tex=%5Chat+p%3D2%5E%7B-%5CDelta%2F%5Chat+h%7D\" alt=\"\\hat p=2^{-\\Delta/\\hat h}\" eeimg=\"1\"/> ：回忆概率</li></ul><p data-pid=\"xZ0gbkgO\">其中</p><ul><li data-pid=\"-JGAs4Wu\"><img src=\"https://www.zhihu.com/equation?tex=%5CDelta\" alt=\"\\Delta\" eeimg=\"1\"/> ：复习间隔</li><li data-pid=\"IS_93zmY\"><img src=\"https://www.zhihu.com/equation?tex=%5Cvec%5Ctheta\" alt=\"\\vec\\theta\" eeimg=\"1\"/> ：学习权重，与特征向量一一对应</li></ul><p data-pid=\"TgU-De6o\">在 HLR 的预测过程中，先用特征和权重计算半衰期，再用复习间隔和半衰期计算回忆概率，从而预测记忆的情况。</p><h2>训练</h2><p data-pid=\"VFimymFH\">这个部分需要机器学习基础，所以就不详细介绍了，简单讲讲两个重要的部分。</p><h3>损失函数</h3><p data-pid=\"2tTmlJKS\"><img src=\"https://www.zhihu.com/equation?tex=loss%3D%28p-%5Chat+p%29%5E2+%2B+%5Calpha%28h-%5Chat+h%29%5E2+%2B+%5Clambda%7C%7C%5Cvec%5Ctheta%7C%7C%5E2\" alt=\"loss=(p-\\hat p)^2 + \\alpha(h-\\hat h)^2 + \\lambda||\\vec\\theta||^2\" eeimg=\"1\"/> </p><p data-pid=\"fZLCFC0t\">可以看出该算法同时优化 p 和 h 的误差，并加入的正则化项，偏好权重更小的模型。</p><h3>数值优化</h3><p data-pid=\"A3l4vSRa\">HLR 所用的数值优化算法是 AdaGrad，比较重要的两个公式是：</p><p data-pid=\"Vuske_TB\">梯度：</p><p data-pid=\"Vdys3x2T\"><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial%5Ctheta_k%7D%3D%26%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+p%7D%5Cfrac%7B%5Cpartial+p%7D%7B%5Cpartial%5Ctheta_k%7D%2B%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial+h%7D%5Cfrac%7B%5Cpartial+h%7D%7B%5Cpartial%5Ctheta_k%7D+%2B+%5Cfrac%7B%5Cpartial+loss%7D%7B%5Cpartial%5Ctheta_k%7D+%5C%5C+%3D%262%5Cleft%28%5Chat%7Bp%7D_%7B%5CTheta%7D-p%5Cright%29+%5Cln+%5E%7B2%7D%282%29+%5Chat%7Bp%7D_%7B%5CTheta%7D%5Cleft%28%5Cfrac%7B%5CDelta%7D%7B%5Chat%7Bh%7D_%7B%5CTheta%7D%7D%5Cright%29+x_%7Bk%7D+%5C%5C+%2B%262+%5Calpha%5Cleft%28%5Chat%7Bh%7D_%7B%5CTheta%7D%2B%5Cfrac%7B%5CDelta%7D%7B%5Clog_%7B2%7D%28p%29%7D%5Cright%29+%5Cln+%282%29+%5Chat%7Bh%7D_%7B%5CTheta%7D+x_%7Bk%7D+%5C%5C+%2B%262+%5Clambda+%5Ctheta_%7Bk%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} \\frac{\\partial loss}{\\partial\\theta_k}=&amp;\\frac{\\partial loss}{\\partial p}\\frac{\\partial p}{\\partial\\theta_k}+\\frac{\\partial loss}{\\partial h}\\frac{\\partial h}{\\partial\\theta_k} + \\frac{\\partial loss}{\\partial\\theta_k} \\\\ =&amp;2\\left(\\hat{p}_{\\Theta}-p\\right) \\ln ^{2}(2) \\hat{p}_{\\Theta}\\left(\\frac{\\Delta}{\\hat{h}_{\\Theta}}\\right) x_{k} \\\\ +&amp;2 \\alpha\\left(\\hat{h}_{\\Theta}+\\frac{\\Delta}{\\log_{2}(p)}\\right) \\ln (2) \\hat{h}_{\\Theta} x_{k} \\\\ +&amp;2 \\lambda \\theta_{k} \\end{aligned}\" eeimg=\"1\"/> </p><p data-pid=\"AAjYLZWP\">学习率：</p><p data-pid=\"tNgEgb2O\"><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bk%7D%5E%7B%28%2B1%29%7D%3A%3D%5Ctheta_%7Bk%7D-%5Ceta%5Cleft%5B%5Cmathrm%7Bc%7D%5Cleft%28x_%7Bk%7D%5Cright%29%5E%7B-%5Cfrac%7B1%7D%7B2%7D%7D%5Cright%5D+%5Cfrac%7B%5Cpartial+%5Cell%7D%7B%5Cpartial+%5Ctheta_%7Bk%7D%7D\" alt=\"\\theta_{k}^{(+1)}:=\\theta_{k}-\\eta\\left[\\mathrm{c}\\left(x_{k}\\right)^{-\\frac{1}{2}}\\right] \\frac{\\partial \\ell}{\\partial \\theta_{k}}\" eeimg=\"1\"/> </p><h2>算法思路总结</h2><ul><li data-pid=\"HbS7YHJ7\">HLR 算法假设回忆概率是关于实际间隔的指数函数。</li><li data-pid=\"nzgM4zkB\">由于 p 是 h 和 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta\" alt=\"\\Delta\" eeimg=\"1\"/> 的函数，而 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta\" alt=\"\\Delta\" eeimg=\"1\"/> 是输入值，所以算法的主要优化对象就是 h。</li><li data-pid=\"IqJ3ZmcB\">由于 h 是模型的一个中间变量，在实际生活中无法直接观测，所以使用 <img src=\"https://www.zhihu.com/equation?tex=h%3D%5Cfrac%7B%E2%88%92%5CDelta%7D%7B%5Clog_2%28p%29%7D\" alt=\"h=\\frac{−\\Delta}{\\log_2(p)}\" eeimg=\"1\"/> 来近似实际值。所以实际上只有 p 是标签（Label）。</li><li data-pid=\"8nLRoPQ4\">模型的损失函数使用的是平方损失函数，并且同时对 p 和 h 同时惩罚。</li><li data-pid=\"43l9ULES\">最后使用 AdaGrad（自适应梯度算法）来优化权重。</li></ul><h2>特征工程</h2><p data-pid=\"LPV9k5Ie\">没有线代、高数基础的读者看到这些数学公式可能会很晕，不过可以放心的是，数学优化不是本期的重点。机器学习最重要的，其实是数据。数据决定了模型的上限。该谈谈标题里提到的特征工程了，其实这也是机器学习的艺术所在。</p><h2>HLR 特征分析</h2><p data-pid=\"GH_ohLjU\">HLR 选用的两个核心特征——right 和 wrong，是非常符合直觉的——right 越大，回忆正确次数越多，自然会记得越牢，半衰期长；wrong 越大，遗忘次数越多，记得不好，半衰期短。</p><p data-pid=\"Ulp_sjrk\">事实上，如果学习者完全按照算法的间隔来复习，并且算法的间隔不会临时调整，这两个特征就非常好了。但是现实并没有那么理想。让我们看看以下四个案例，就会发现这样提取特征的问题：</p><ol><li data-pid=\"u96Aq56n\">案例一：复习间隔为 1 3 5，反馈为 正确 正确 错误</li><li data-pid=\"tPkM-9ud\">案例二：复习间隔为 1 3 5，反馈为 错误 正确 正确</li><li data-pid=\"fxn-aE51\">案例三：复习间隔为 2 6 10，反馈为 正确 正确 错误</li><li data-pid=\"VBXBrBp5\">案例四：复习间隔为 2 6 10，反馈为 错误 正确 正确</li></ol><p data-pid=\"Th2qs9tS\">通过计算，我们会发现，它们的 right 和 wrong 都一样。但是显然，它们的复习过程区别很大（后两个案例的间隔前两个案例的两倍，另外还有反馈顺序的问题）。这种区别在特征提取的过程中被忽略了。</p><p data-pid=\"DLRvkLMT\">当然，特征工程本来就是要忽略区别，找到一个合适的粒度，便于模型找到规律。是否忽略某些区别，就要看这种区别对预测的影响是否显著。</p><p data-pid=\"k240cKtB\">之前我在参数模拟<sup data-text=\"【硬核】参数模拟——每天 40 张新卡片，365 天后我要复习多少？\" data-url=\"https://zhuanlan.zhihu.com/p/78398403\" data-draft-node=\"inline\" data-draft-type=\"reference\" data-numero=\"3\">[3]</sup>那篇文章提到过，两倍的间隔在理论上会将回忆概率从 90% 降低到 80%。对于一个百分数来说，10% 的差距可以说是非常大了。所以，我认为这种区别是不应该被忽略的。</p><h2>状态空间方法特征分析</h2><p data-pid=\"IYGNWnO8\">既然分析机器学习方法的特征，不妨也把状态空间方法拿出来对比对比。</p><p data-pid=\"744y6cew\">以 Anki 为例，用到的特征只有 grade 和 t，也就是反馈和复习间隔。看起来这两个特征比 HLR 用的豪华特征集寒碜多了。但是我们别忘了状态空间本身的特点：状态。</p><p data-pid=\"CzgA4SOZ\">没错，其实状态在一定程度上，保留了记忆的反馈历史。每一次状态转移都是基于反馈和复习间隔的。所以，在这里，状态空间方法用到了被机器学习方法省略的历史间隔特征和反馈顺序。</p><p data-pid=\"ZJUyG2zH\">那可能就会有读者问了，这只是特征工程的锅，盖不到机器学习方法上。确实，这只是特征选取的问题。但是想要在机器学习方法上用到历史间隔和反馈顺序，还是有一定难度的，有两种方法比较常见。</p><h2>如何在机器学习中利用记忆历史反馈信息（时间序列）？</h2><p data-pid=\"1E16blBv\">第一种方法，比较暴力。直接将历史间隔和反馈顺序以矩阵的方式输入。比如我们以天为单位，并用 1 表示正确，0 表示错误。那么案例四的特征可以用两个向量表示： <img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Bhistory_space%7D+%3D+%5B2%2C+4%2C+6%5D\" alt=\"\\vec{history_space} = [2, 4, 6]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Bhistory_grade%7D+%3D+%5B0%2C+1%2C+1%5D\" alt=\"\\vec{history_grade} = [0, 1, 1]\" eeimg=\"1\"/> 。</p><p data-pid=\"Q2k33rdd\">这两个向量包含了历史间隔和反馈顺序的完整信息，看起来非常不错。然而，如果真要这样做特征工程，会遇到一个很大的问题：数据稀疏。</p><p data-pid=\"JkylN2fG\">什么意思呢？让我们看看，这是复习了 3 次之后的记录，就有 6 个值了。其中反馈还好，只有 2 * 2 * 2 = 8 种可能的结果，但是间隔可能会有 10 * 10 * 10 = 1000 种可能的结果（这还是把间隔上限设置为 10 天的情况）。如果复习 10 次，这个可能结果的数量就要组合爆炸了。再多的样本也难以填满这个样本空间。而数据稀疏带来的后果就是误差偏大、难以训练等等。</p><p data-pid=\"sNitnDqY\">第二种方法，那就是给模型增加记忆能力，比如循环神经网络、自回归模型等。这种方法其实是在向状态空间方法学习，将过去的输出作为当前的输入，能够更好地处理时序数据。</p><p data-pid=\"NpcvK28u\">不过这种方法也存在一些问题，倒不是理论上的问题，而是实践上的问题。因为目前为止还没有相关研究用这种方法来处理记忆数据。并且应用这种方法该如何做数据处理也是一个新的挑战。</p><h2>总结</h2><p data-pid=\"ump6GFCC\">一不小心写得太长了，先感谢一下能看到这的读者朋友们 2333</p><p data-pid=\"zwBktHAf\">本文介绍了 Duolingo 所使用的 HLR 算法，概述了机器学习方法是如何应用于记忆预测的。同时深入探究了记忆反馈数据的特征工程和利用方法，简单提及了几个可能的解决方法及其难点。联系前几期文章，我认为记忆模型的进一步发展还需要依靠回归模型和状态空间相结合的方法。</p><p data-pid=\"8BXjiQxZ\">记忆研究还有很多问题等待我们解决，希望我们能够解开记忆的迷雾，让记忆的规律更加清晰地浮现出来。</p><p data-pid=\"AA82CATb\">这里是热爱记忆算法研究的学委叶哥，欢迎大家在评论区交流！</p><p class=\"ztext-empty-paragraph\"><br/></p><p data-pid=\"a3OsRF1g\">2021 年 1 月 18 日</p><p data-pid=\"xh60Ss8W\">叶峻峣</p>",
  "column": {
    "updated": 1706022041,
    "description": "",
    "author": {
      "is_followed": false,
      "badge": [],
      "name": "Thoughts Memo",
      "url": "https://api.zhihu.com/people/4c592f496dc33822b560b382907ff1d0",
      "gender": 1,
      "user_type": "people",
      "headline": "学校≠教育≠技能；文凭溢价=80%信号传递+20%人力资本",
      "avatar_url": "https://picx.zhimg.com/50/v2-41f893b0cd84fb6a8c8f6d1bd29c9554_l.jpg?source=b1748391",
      "is_following": false,
      "type": "people",
      "id": "4c592f496dc33822b560b382907ff1d0"
    },
    "url": "https://api.zhihu.com/columns/c_1280249768422608896",
    "title": "学委叶哥的随笔",
    "image_url": "https://picx.zhimg.com/v2-f111d7ee1c41944859e975a712c0883b_720w.jpg?source=d16d100b",
    "type": "column",
    "id": "c_1280249768422608896"
  },
  "comment_count": 20,
  "image_url": "https://pic1.zhimg.com/v2-60f126e4be8f5243decfb2fbf70f9afd_720w.jpg?source=b1748391",
  "linkbox": {
    "url": "",
    "category": "",
    "pic": "",
    "title": ""
  },
  "voting": 0,
  "type": "article",
  "suggest_edit": {
    "status": false,
    "url": "",
    "reason": "",
    "tip": "",
    "title": ""
  },
  "is_normal": true,
  "censored": false
}