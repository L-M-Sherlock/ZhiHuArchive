06 1989: SuperMemo 适应用户记忆(下)
<p>下篇主要从数据和理论上证明了 SM-5 算法对 SM-2 算法的优越性。本来还有后续的算法优化内容，介于其过于硬核，就不搬到知乎上了，感兴趣的朋友可以直接看我的电子书：</p><a href="http://link.zhihu.com/?target=https%3A//www.kancloud.cn/ankigaokao/supermemo-guru-cn/1895505" data-draft-node="block" data-draft-type="link-card" class=" wrap external" target="_blank" rel="nofollow noreferrer">间隔重复的历史</a><hr><h3><b>Evaluation of SuperMemo 5 (1989)</b></h3><h3><b>SuperMemo5 的评估（1989）</b></h3><p>SuperMemo 5 was so obviously superior that I did not collect much data to prove my point. I made only a few comparisons for my <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Master%2527s_Thesis" class=" wrap external" target="_blank" rel="nofollow noreferrer">Master's Thesis</a></u> and they left no doubt.</p><p>SuperMemo 5的优势如此明显，以至于我没有收集太多数据来证明我的观点。我只拿我的<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Master%2527s_thesis" class=" wrap external" target="_blank" rel="nofollow noreferrer">硕士论文</a></u>做了几次比较，结果毫无疑问。</p><p>Archive warning: <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">Why use literal archives?</a></u></p><p>档案警告:<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用文字档案?</a></u></p><p>This text was part of: "<i>Optimization of learning</i>" by <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a></u> (1990)</p><p>本文是<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a></u>(1990)的《<i>优化学习</i>》的一部分。</p><p><b>3.8. Evaluation of the Algorithm SM-5</b></p><p><b>3.8.算法 SM-5 的评估</b></p><p>The Algorithm SM-5 has been in use since October 17, 1989, and has surpassed all expectations in providing an efficient method of determining the desired function of <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">optimal intervals</a></u>, and in consequence, improving the <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Acquisition_rate" class=" wrap external" target="_blank" rel="nofollow noreferrer">acquisition rate</a></u> (15,000 items learnt within 9 months). Fig. 3.5 indicates that the acquisition rate was at least twice as high as that indicated by combined application of the <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-2" class=" wrap external" target="_blank" rel="nofollow noreferrer">SM-2</a></u> and <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-4" class=" wrap external" target="_blank" rel="nofollow noreferrer">SM-4</a></u> algorithms!</p><p>算法 SM-5 自 1989 年 10 月 17 日开始使用，它提供了一种确定理想的最优间隔函数的有效方法，超出了所有人的预期，从而提高了习得率(在 9 个月内学习了 15,000 个项目)。图 3.5 显示习得速率至少是组合应用 SM-2 和 SM-4 算法的两倍！</p><figure data-size="normal"><img src="https://pic1.zhimg.com/v2-8c281fa742eee9a032df9a090587ab6d_720w.jpg?source=3af55fa1" data-caption="" data-size="normal" data-rawwidth="407" data-rawheight="600" class="content_image" width="407"></figure><blockquote><b>Figure:</b> Changes of the work burden in databases supervised by SM-2 and SM-5 algorithms.<br><b>图片：</b>在 SM-2 和 SM-5 算法的监督下，数据库中工作负担的变化</blockquote><p>The knowledge retention increased to about 96% for 10-month-old databases. Below, some knowledge retention data in selected databases are listed to show the comparison between the SM-2 and SM-5 algorithms:</p><p>对于 10 个月的数据库，知识保留率提高到 96% 左右。下面列出了选定数据库中的一些知识保留率数据，以显示 SM-2 和 SM-5 算法之间的比较：</p><ul><li>Date - date of the measurement,</li><li>日期 - 测量日期，</li><li>Database - name of the database; ALL means all databases averaged</li><li>数据库 - 数据库的名称；ALL 表示所有数据库的平均值</li><li>Interval - average current interval used by items in the database</li><li>间隔 - 数据库中项目使用的平均当前间隔</li><li>Retention - knowledge retention in the database</li><li>保留率 - 数据库中的知识保留率</li><li>Version - version of the algorithm applied to the database</li><li>版本 - 数据库所应用的算法版本</li></ul><p><b>draft-type="table" data-size="normal" data-row-style="normal"&gt;</b></p><b><table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"><tbody><tr><th></th><th></th><th></th><th></th><th></th></tr><tr><td>Dec 88</td><td>EVD</td><td>17 days</td><td>81%</td><td>SM-2</td></tr><tr><td>Dec 89</td><td>EVG</td><td>19 days</td><td>82%</td><td>SM-5</td></tr><tr><td>Dec 88</td><td>EVC</td><td>41 days</td><td>95%</td><td>SM-2</td></tr><tr><td>Dec 89</td><td>EVF</td><td>47 days</td><td>95%</td><td>SM-5</td></tr><tr><td>Dec 88</td><td>all</td><td>86 days</td><td>89%</td><td>SM-2</td></tr><tr><td>Dec 89</td><td>all</td><td>190 days</td><td>92%</td><td>SM-2, SM-4 and SM-5</td></tr></tbody></table></b><p>In the process of repetition the following distribution of grades was recorded:</p><p>在复习过程中，记录了下列成绩分布情况:</p><p><b>ft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"&gt;</b></p><b><table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"><tbody><tr><th>Quality</th><th>Fraction</th></tr><tr><td>0</td><td>0%</td></tr><tr><td>1</td><td>0%</td></tr><tr><td>2</td><td>11%</td></tr><tr><td>3</td><td>18%</td></tr><tr><td>4</td><td>26%</td></tr><tr><td>5</td><td>45%</td></tr></tbody></table></b><p>This distribution, in accordance to the assumptions underlying the Algorithm SM-5, yields the average <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Grade" class=" wrap external" target="_blank" rel="nofollow noreferrer">response quality</a></u> equal to 4. The <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">forgetting index</a></u> equals to 11% (<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Item" class=" wrap external" target="_blank" rel="nofollow noreferrer">items</a></u> with <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Grade" class=" wrap external" target="_blank" rel="nofollow noreferrer">quality</a></u> lower than 3 are regarded as forgotten). Note, that the retention data indicate that only 4% of items in a database are not remembered. Therefore forgetting index exceeds the percentage of forgotten items 2.7 times.</p><p>根据算法 SM-5 的假设，该分布产生的平均回忆质量等于 4。遗忘指数等于 11%(成绩低于 3 的项目被认为是被遗忘的)。请注意，保留率数据表明数据库中只有 4% 的项目没有被记住。因此，遗忘指数超过遗忘项目百分比的 2.7 倍。</p><p>In a 7-month old database, it was found that 70% of items had not been forgotten even once in the course of repetitions preceding the measurement, while only 2% of items had been forgotten more than 3 times</p><p>在一个 7 个月前的数据库中，发现 70% 的项目在测量之前的重复过程中甚至没有忘记一次，而只有 2% 的项目遗忘次数超过 3 次</p><h3><b>Theoretic proof of superiority of newer algorithms</b></h3><h3><b>新算法优越性的理论证明</b></h3><p>Anki's criticism of SuperMemo 5 calls for a simple proof in the light of modern <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Spaced_repetition" class=" wrap external" target="_blank" rel="nofollow noreferrer">spaced repetition</a></u> theory. We can show that today's model of memory can be mapped onto the models underlying both algorithms: <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-2" class=" wrap external" target="_blank" rel="nofollow noreferrer">Algorithm SM-2</a></u> and <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-5" class=" wrap external" target="_blank" rel="nofollow noreferrer">Algorithm SM-5</a></u>, and the key difference between the two is the missing adaptability of the function of <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">optimal intervals</a></u> (represented in Algorithm SM-5 by <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Matrix_of_optimum_factors" class=" wrap external" target="_blank" rel="nofollow noreferrer">matrix of optimum factors</a></u>).</p><p>Anki 对 SuperMemo 5 的批评需要根据现代的<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Spaced_repetition" class=" wrap external" target="_blank" rel="nofollow noreferrer">间隔重复</a></u>理论来做一个简单的证明。我们可以表明，今天的记忆模型可以映射到两种算法基础上的模型：<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-2" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-2</a></u> 和<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-5" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-5</a></u>，两者之间的关键区别是最优间隔函数的适应性缺失(在算法SM-5中由最优因子矩阵表示)。</p><p>Let SInc=f(C,S,R) be a <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability_increase" class=" wrap external" target="_blank" rel="nofollow noreferrer">stability increase</a></u> function that takes <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Complexity" class=" wrap external" target="_blank" rel="nofollow noreferrer">complexity</a></u> C, <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability" class=" wrap external" target="_blank" rel="nofollow noreferrer">stability</a></u> S, and <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retrievability" class=" wrap external" target="_blank" rel="nofollow noreferrer">retrievability</a></u> R as arguments. This function determines the progressive increase in review <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">intervals</a></u> in optimum learning.</p><p>SInc = f (C、S、R) 是一个<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability_increase" class=" wrap external" target="_blank" rel="nofollow noreferrer">稳定性增长</a></u>函数，<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Complexity" class=" wrap external" target="_blank" rel="nofollow noreferrer">复杂性</a></u> C、<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability" class=" wrap external" target="_blank" rel="nofollow noreferrer">稳定</a></u> S、<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retrievability" class=" wrap external" target="_blank" rel="nofollow noreferrer">可恢复性</a></u> R 作为参数。这个函数决定了最优学习中复习<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">间隔</a></u>的递增。</p><p>Both Algorithms, SM-2 and SM-5 ignore the <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retrievability" class=" wrap external" target="_blank" rel="nofollow noreferrer">retrievability</a></u> dimension. In theory, if both algorithms worked perfectly, we could assume they aim at R=0.9. As it can be measured in <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a></u>, both algorithms fail at that effort for they do not know relevant <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_curve" class=" wrap external" target="_blank" rel="nofollow noreferrer">forgetting curves</a></u>. They simply do not collect forgetting curve data. This data-collecting possibility was introduced only in <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-6" class=" wrap external" target="_blank" rel="nofollow noreferrer">Algorithm SM-6</a></u> in 1991.</p><p>两种算法，SM-2 和 SM-5 都忽略了<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retrievability" class=" wrap external" target="_blank" rel="nofollow noreferrer">可提取性</a></u>维度。理论上，如果两种算法都能完美运行，我们可以假设它们的目标是 R=0.9。正如可以在 <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a></u> 中测量的那样，这两种算法都失败了，因为它们不知道相关的<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_curve" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘曲线</a></u>。他们只是不收集遗忘曲线数据。这种数据收集的可能性是在 1991 年才在<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-6" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-6</a></u> 中引入的。</p><p>However, if we assume that <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/The_birthday_of_spaced_repetition%3A_July_31%2C_1985" class=" wrap external" target="_blank" rel="nofollow noreferrer">1985</a></u> and <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo_1.0_for_DOS_%281987%29" class=" wrap external" target="_blank" rel="nofollow noreferrer">1987</a></u> heuristics were perfect guesses, in theory, the algorithm could use SInc=F(C,S) with constant R of 90%.</p><p>然而，如果我们假设 <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/The_birthday_of_spaced_repetition%3A_July_31%25EF%25BC%258C_1985" class=" wrap external" target="_blank" rel="nofollow noreferrer">1985</a></u> 和 <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo_1.0_for_DOS_%281987%29" class=" wrap external" target="_blank" rel="nofollow noreferrer">1987</a></u> 启发式是完美的猜测，在理论上，该算法可以使用SInc=F(C,S)，常数R为90%。</p><p>Due to the fact that SM-2 uses the same number, <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/E-factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">EF</a></u> for <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability_increase" class=" wrap external" target="_blank" rel="nofollow noreferrer">stability increase</a></u> and for item <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Complexity" class=" wrap external" target="_blank" rel="nofollow noreferrer">complexity</a></u>, for SM-2 we have SInc=f(C,S) equation represented by EF=f'(EF,interval), where it can be shown easily with data that f&lt;&gt;f'. Amazingly, the heuristic used in SM-2 made this function work by decoupling the actual link between the EF and item <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Complexity" class=" wrap external" target="_blank" rel="nofollow noreferrer">complexity</a></u>. As data shows that <i>SInc</i> keeps decreasing with an increase in <i>S</i>, in <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-2" class=" wrap external" target="_blank" rel="nofollow noreferrer">Algorithm SM-2</a></u>, by definition, all items would need to gain on complexity with each review if EF was to represent item complexity. In practical terms, Algorithm SM-2 uses EF=f'(EF,interval), which translates to SInc(n)=f(SInc(n-1),interval).</p><p>由于 SM-2 使用相同的数字 EF 用于稳定性增加和项目复杂性，对于 SM-2，我们有以 EF=f'(EF,interval) 的形式表示的 SInc=f(C,S) 方程，其中的数据可以很容易地显示 f&lt;&gt;f' 。令人惊讶的是，SM-2 中使用的启发式通过解耦 EF 和项目复杂性之间的实际联系，使这个函数发挥作用。由于数据显示 SInc 随着 S 的增加而不断减少，在算法 SM-2 中，根据定义，如果要用 EF 来表示项的复杂度，那么所有项都需要在每次复习时获得复杂度。在实际应用中，算法 SM-2 使用 EF=f'(EF,interval)，即 SInc(n)=f(SInc(n-1),interval)。</p><p>Let us assume that the EF=f(EF,interval) heuristic was an excellent guess as claimed by proponents of Algorithm SM-2. Let <i>SInc</i> be represented by <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/O-factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">O-factor</a></u> in Algorithm SM-5. We might then represent <i>SInc</i>=f(C,S) as <i>OF</i>=f(<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/EF" class=" wrap external" target="_blank" rel="nofollow noreferrer">EF</a></u>,<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">interval</a></u>).</p><p>让我们假设 EF=f(EF,interval) 启发式正如 SM-2 算法的支持者所声称的那样，是一个很好的猜想。令 <i>SInc</i> 在算法 SM-5 中由 <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/O-factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">O-factor</a></u> 表示。然后我们可以将 <i>SInc</i>=f(C,S) 表示为 <i>OF</i>=f(<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/EF" class=" wrap external" target="_blank" rel="nofollow noreferrer">EF</a></u>,<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">interval</a></u>)。</p><p>For Algorithm SM-2, OF would be constant and equal to EF, in Algorithm SM-5, OF is adaptable and can be modified depending on algorithm's performance. It seems pretty obvious that penalizing the algorithm for bad performance by a drop to <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/OF_matrix" class=" wrap external" target="_blank" rel="nofollow noreferrer">OF matrix</a></u> entries, and rewarding it by an increase in OF entries is superior to keeping OF constant.</p><p>对于算法 SM-2, OF 是常数，等于 EF，在算法 SM-5 中，OF是可适应的，可以根据算法的表现进行修改。很明显，对表现差的算法进行惩罚，降低<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/OF_matrix" class=" wrap external" target="_blank" rel="nofollow noreferrer">OF 矩阵</a></u>对应的项，并通过增加对应的项来奖励它，这要优于保持不变。</p><p>On a funny twist, as much as supporters of Algorithm SM-2 claim it performs great, supporters of neural network SuperMemo kept accusing algebraic algorithms of: lack of adaptability. In reality, the adaptability of <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17" class=" wrap external" target="_blank" rel="nofollow noreferrer">Algorithm SM-17</a></u> is best to-date as it is based on the most accurate <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Three_component_model_of_memory" class=" wrap external" target="_blank" rel="nofollow noreferrer">model of memory</a></u>.</p><p>有趣的是，尽管 SM-2 算法的支持者声称它表现得很好，但神经网络 SuperMemo 算法的支持者却不断指责代数算法：缺乏适应性。实际上，<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-17</a></u> 的适应性是最好的，因为它是基于最精确的<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Three_component_model_of_memory" class=" wrap external" target="_blank" rel="nofollow noreferrer">记忆模型</a></u>。</p><p>It is conceivable that heuristics used in SM-2 were so accurate that the original guess on <i>OF</i>=f(EF,interval) needed no modification. However, as it has been shown in practical application, the <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/OF_matrix" class=" wrap external" target="_blank" rel="nofollow noreferrer">OF matrix</a></u> quickly evolves, and converges onto values described in <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/ANE1994" class=" wrap external" target="_blank" rel="nofollow noreferrer">this publication (Wozniak, Gorzelanczyk 1994)</a></u>. They differ substantively from the assumption wired into <u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-2" class=" wrap external" target="_blank" rel="nofollow noreferrer">Algorithm SM-2</a></u>.</p><p>可以想象，在 SM-2 中使用的启发法是如此精确，以致于原来对 <i>OF</i>=f(EF,interval) 的猜测不需要修改。然而，正如在实际应用中所显示的那样，<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/OF_matrix" class=" wrap external" target="_blank" rel="nofollow noreferrer">OF 矩阵</a></u>迅速发展，并收敛到<u><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/ANE1994" class=" wrap external" target="_blank" rel="nofollow noreferrer">这篇出版物(Wozniak, Gorzelanczyk 1994)</a></u>中描述的值。它们与算法 SM-2 中的假设有本质上的不同。</p><p><b>Summary:</b></p><p><b>总结：</b></p><ul><li>sm17 (2016): SInc=f(C,S,R), 3 variables, f is adaptable</li><li>sm17 (2016): SInc=f(C,S,R)， 3 个变量，f 是有适应性的</li><li>sm5 (1989): SInc=f(C,S,0.9), 2 variables, f is adaptable</li><li>sm5 (1989): SInc=f(C,S,0.9)， 2 个变量，f 是有适应性的</li><li>sm2 (1987): SInc=f(SInc,S,0.9) - 1 variable, f is fixed</li><li>sm2 (1987): SInc=f(SInc,S,0.9) - 1 个变量，f 是固定的</li></ul><hr><p>从这篇开始，SuperMemo 的算法会越来越精致，也越来越硬核，所以不会翻译的太详细，想了解的朋友可以把我的翻译当作参考，具体细节还请看英文原文。</p>