<meta charset="UTF-8">
07 1990：记忆的通用公式
<p data-pid="fSRXyGFa">开始填这个史前大坑了</p><a href="https://zhuanlan.zhihu.com/p/375379522" data-draft-node="block" data-draft-type="link-card" data-image="https://pic1.zhimg.com/v2-e2f3e31b3b37b0c9b2abe6171cf5329f_qhd.jpg?source=3af55fa1" data-image-width="1199" data-image-height="854" class="internal">叶峻峣：0 Content 目录《间隔重复的历史》</a><p data-pid="DJHkZmQy">上一章：<a href="https://zhuanlan.zhihu.com/p/213170503" class="internal">06 1989: SuperMemo 适应用户记忆(下) - 知乎 (zhihu.com)</a></p><hr><blockquote data-pid="mz0oVsMv"><a href="http://link.zhihu.com/?target=https%3A//paratranz.cn/projects/3131" class=" wrap external" target="_blank" rel="nofollow noreferrer">Thoughts Memo</a> 汉化组译制</blockquote><p data-pid="JaCcifFA">原文：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/History_of_spaced_repetition_%28print%29%231990%3A_Universal_formula_for_memory" class=" wrap external" target="_blank" rel="nofollow noreferrer">1990: Universal formula for memory</a></p><h2>最优复习与间歇复习</h2><p data-pid="g0MWGO9T">到了 1990 年，我很是笃定我手握着重大发现。我破解了<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘</a>难题。我知道了记忆简单内容的复习<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Spaced_repetition" class=" wrap external" target="_blank" rel="nofollow noreferrer">最佳时机</a>。一经在我<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">硕士论文</a>中描述它的许可，我的探索欲也水涨船高。我希望我可以找到一个长期记忆的通用公式，能够让我跟踪记忆在各种形式的检索和接触中的表现形式。</p><p data-pid="GvXmUT93">我已经收集了一些数据，这些数据可能会帮助我找到这个公式。在 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Birth_of_SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">1985</a> 年发现最佳重复间隔之前，我把问题写在一页页纸上来复习知识。此时复习混乱不堪，由可利用的时间、需要或心情支配。我把这称为「间歇性学习」。我有单个页面和每次复习的回忆数据。虽没有 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a> 的周期性，这种数据也算较为理想。这正是我为解决记忆问题而需要的数据。只不过，这些数据都仅仅记录在纸上。</p><p data-pid="FmRgRUUF">1990 年春天，我叫我姐姐来打字录入数据。当然，我没有一个妹妹会情愿来做这件事。我的姐姐比我大 17 岁。我利用她对我的爱，让她做这种枯燥繁重的工作，而没有顾及到她的时间。她两年后去世了。我再也没有机会报答她对<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Spaced_repetition" class=" wrap external" target="_blank" rel="nofollow noreferrer">间隔重复</a>理论的贡献，她甚至没有机会了解这个理论。从 1990 年 5 月 1 日开始，她在我不用电脑的时间，将数据从纸上转移到电脑上。她打字很慢，花了很多天。她的工夫是值得的。</p><h2>间歇学习模型</h2><p data-pid="udo5luTg">在 1990 年的整个夏天，我没有专注于我的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">硕士论文</a>，而是研究「间歇学习模型」。对我来说，连续工作 10 个小时，感觉没有半点发现而在早上 7 点睡觉，或者让电脑整夜计算数据，都是很正常的。</p><p data-pid="DpnjyNmt">锲而不舍，捣鼓调整是有代价的。只有少年才能负担得起，他们应该有空间和自由。尽管我已经 28 岁了，家人们还是默默忍受我的一举一动。就像一个不成熟的青少年。我住在我姐姐的公寓里，在那里我可以利用她的善意。在电脑前工作的时间很长，被借口为「在做我的硕士论文」。事实上，没有人要求我这样做，也没有人要求我这样做，它甚至没有推动 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a> 的发展。这是一个纯粹的科学好奇心的案例。我只是想知道记忆是如何工作的。</p><p data-pid="_HA8FV8b">我有几十页的问题和他们的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Repetition_history" class=" wrap external" target="_blank" rel="nofollow noreferrer">重复历史</a>。我试图预测「每页的记忆失误」。我使用<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Deviation" class=" wrap external" target="_blank" rel="nofollow noreferrer">平方根标准差</a>来预测失误（下面表示为Dev）。到 1990 年 7 月 10 日，星期二，我达到了 Dev&lt;3，感觉问题几乎「解决」了。1990 年 7 月 12 日，我改进到 Dev=2.877（顺便说一下，我的论文中提到了2.887241）。然而，到 1990 年 8 月 27 日，我在那天的笔记中宣布这个问题无法解决。</p><hr><p data-pid="juqLqtSx"> 个人轶事。<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_anecdotes%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用轶事？</a><br><br>1990 年 8 月 27 日：<b>我解决了</b>间歇学习，<b>表明这个问题是无法解决的</b>！单独一个参数无力描述与整页项目的记忆强度。这表明，<b><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/E-Factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">E 因子</a>较低的项目没有</b><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">最优间隔</a>！</p><hr><p data-pid="InS1GUnB">1990 年 8 月 30 日，我在硕士论文中解说了这个模型。文章一共有 15 页，不算很好读。我打赌没有人有耐心读完整篇文章。90 年代末我的硕士论文节选版发布在网络，而描述间歇学习的这一章甚至没有在 <a href="http://link.zhihu.com/?target=http%3A//supermemo.com/" class=" wrap external" target="_blank" rel="nofollow noreferrer">supermemo.com</a> 上发表。</p><p data-pid="dFwxQfSx">然而，基于该模型得出的结论，深刻地影响了我随后几十年中对记忆的思考。该模型背后的想法，实际上非常类似我在开发 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17" class=" wrap external" target="_blank" rel="nofollow noreferrer">SM-17 算法</a> (2014-2016) 时应用的优化。</p><p data-pid="2A2kLr77">当我宣布这个问题无法解决时，我的意思是我无法准确地描述「困难页」的记忆，因为性质不同的材料需要更复杂的模型。然而，这篇 1990 年 8 月 31 日记录的笔记却对此更加乐观：</p><hr><p data-pid="TUyiEO_i"> 个人轶事。<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_anecdotes%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用轶事？</a><br><br>1990 年 8 月 31 日：不间断地研究间歇学习模型。到了晚上，计算机终究没有让我离解决方案更进一步。然而，我有个好主意，就是用 IL 模型的绝世优秀功能计算出最优间隔。屏幕上的结果映入眼帘时，我简直不敢相信我 [哔——] 看到什么。这些正是我在 1985 年<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Birth_of_SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">发现</a>时试图制定 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a> 方法时发现的间隔。我高兴地在家里跳来跳去，简直像一条有两条尾巴的狗。所以我可以说我真的解决了 IL 问题（对比 1990 年 8 月 27 日）。但我发现，这个成功并不是今天给我发现的一切：</p><ul><li data-pid="WCxb9snv"><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">最优系数</a>随着连续的间隔而减少（我以前凭直觉感觉是这样的），</li><li data-pid="JckgQ6VM"> 对于<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>等于 10%，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retention" class=" wrap external" target="_blank" rel="nofollow noreferrer">保留率</a>为 94%（如 EVF <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Database" class=" wrap external" target="_blank" rel="nofollow noreferrer">数据库</a>）</li><li data-pid="JWcyXLL-"><b><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retention" class=" wrap external" target="_blank" rel="nofollow noreferrer">保留率</a>与<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a></b>呈线性关系 [2018 年评论：在异质材料的小范围内]<b>（这无法从我 1 月份进行的模拟实验中计算出来）</b></li><li data-pid="HC4cI98Y"> 该模型说，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>的理想值是 5-10%（工作量-保留率的权衡）</li><li data-pid="OkzKScNQ"> 如果间隔时间是最优时间的两倍，则<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability" class=" wrap external" target="_blank" rel="nofollow noreferrer">记忆强度</a>增加最多！！</li><li data-pid="ib5jCGE2"> 如果<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>为 20%，则<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability" class=" wrap external" target="_blank" rel="nofollow noreferrer">记忆强度</a>增加最多</li></ul><p data-pid="TJDkSQZ5"> [...] 我的公式只有在间隔比以前的强度短不了多少时才有效。</p><hr><h2>过去（1990）与现在（2018）的对比</h2><p data-pid="ScFgTUWS">本章末尾的结论，以及程序本身都让人想起我在 2005 年寻找<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability" class=" wrap external" target="_blank" rel="nofollow noreferrer">记忆稳定性</a>提高的通用公式时使用的方法，以及在 2014 年，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-17</a> 是基于对记忆的更精确的数学描述。像最新的 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a> 算法一样，该模型使得计算任何重复计划的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retention" class=" wrap external" target="_blank" rel="nofollow noreferrer">保留率</a>成为可能。当然，它的准确度要低得多，因为它基于劣质的数据。此外， <a href="http://link.zhihu.com/?target=http%3A//super-memo.com/supermemo17.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo 17</a> 所做的实时工作，在 1990 年时需要花费许多小时的 PC 电脑时间。</p><p data-pid="sCbP0C5H">我的硕士论文中这个看似无聊的老的部分到现在已经变得很重要了。我敢说，只有劣质的数据将这项工作与 25 年后出现的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-17</a> 相隔甚远。我引用的这段文字在符号和文体上做了些许改进，但没有关于<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_curve" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘曲线</a>的章节，该章节由于计算中使用的材料太不同而出现错误。</p><hr><p data-pid="TND2kwfp">存档警告：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用文字档案？</a></p><p data-pid="gJxJsS5z">这段文字是《<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">优化学习</a>》 作者：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a> (1990) 的一部分</p><h3><b>间歇学习模型</b></h3><p data-pid="kfXX_LZy"><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a> 模型为计算<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">最优间隔</a>提供了基础，在时间最优的学习过程中，应该把重复的内容分开。</p><p data-pid="2-pur9nc">然而，如果重复的时间间隔不规律，则无法预测记忆变量的变化。</p><p data-pid="0mhr0CZ3">下面我提出一个尝试，以增强 SuperMemo 模型，使其可以用于描述间歇学习的过程。</p><p data-pid="7aP2u8pY">在<a href="http://link.zhihu.com/?target=http%3A//super-memory.com/english/ol/beginning.htm" class=" wrap external" target="_blank" rel="nofollow noreferrer">第三章</a>中，我提到了，在<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo_on_paper" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-0</a> 开发之前，我学习英语和生物的方式。</p><p data-pid="5PGNYLWd">那段时间（1982-1984年）收集的数据为构建间歇学习模型提供了一个很好的基础。遵照<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Item" class=" wrap external" target="_blank" rel="nofollow noreferrer">最小信息原则</a>制定的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Minimum_information_principle" class=" wrap external" target="_blank" rel="nofollow noreferrer">项目</a>（通常有成对的词的形式）被分组在页面中，进行不定期的复习过程。</p><p data-pid="z5d4cZoS">所收集的数据以计算机可读形式提供，包括71页的重复描述，此外，80个类似的页面参与了由<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo_on_paper" class=" wrap external" target="_blank" rel="nofollow noreferrer">SM-0</a>时间表监督的过程。</p><hr><h2>与算法 SM-17 的相似性</h2><p data-pid="brmtJQ2n">请注意，这个问题的表述让人想起了<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法SM-17</a> 中用来计算<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability_increase" class=" wrap external" target="_blank" rel="nofollow noreferrer">稳定性增长矩阵（SInc[]）</a>的程序。<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability" class=" wrap external" target="_blank" rel="nofollow noreferrer">记忆稳定性</a>被重新缩放，以便能够将其解释为一个<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Interval" class=" wrap external" target="_blank" rel="nofollow noreferrer">间隔</a>。甚至符号也是相似的：S 代表<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability" class=" wrap external" target="_blank" rel="nofollow noreferrer">稳定性</a>，D 代表<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Deviation" class=" wrap external" target="_blank" rel="nofollow noreferrer">偏差</a>。页面遗忘数量代替了<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retrievability" class=" wrap external" target="_blank" rel="nofollow noreferrer">可提取性</a>。</p><p data-pid="-E8dOiNm">我以前喜欢玩各种优化算法。你仍然可以在 <a href="http://link.zhihu.com/?target=http%3A//super-memo.com/supermemo17.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo 17</a> 中查看该算法做表面拟合优化的可视化（见<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability_increase" class=" wrap external" target="_blank" rel="nofollow noreferrer">图片</a>）。12 个变量做处理可能有点低效，但我从不关心处理方法本身如何，只关心能否结果是否可以拓展我对记忆原理的认知。</p><p data-pid="RDU9LCpI">对于那些熟悉<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-17" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-17</a> 的人，我们在下面的文本中改变了符号。此外，我们改变了 In 和 Ln 等符号，这些符号在打印时很容易被误读为对数。</p><p data-pid="yhstNpNy">变化清单：</p><ul><li data-pid="tLjbCiAA"> Ln -&gt; <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1"> </li><li data-pid="X4a9BfRt"> In -&gt; <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> </li><li data-pid="oIYIoRdc"> Dn -&gt; <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BDev_n%7D" alt="\mathrm{Dev_n}" eeimg="1"> </li><li data-pid="aEXRZ6Wj"> R -&gt; RepNo<br> </li></ul><h2>间歇学习问题的表述</h2><hr><p data-pid="R6w4wTGJ">存档警告：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用文字档案？</a></p><p data-pid="Tncq5Hko">这段文字是《<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">优化学习</a>》作者：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a> (1990) 的一部分</p><h3><b>11.1.间歇学习问题的提出</b></h3><p data-pid="atszcVa6">1. 共有 161 个页面。</p><p data-pid="8wiekq68">2. 每页包含约 40 个项目。</p><p data-pid="1Abiqt6G">3. 对于每一页，学习过程的描述（在实验重复期间收集）有以下形式：</p><p data-pid="lohxBlZc"> ((-, <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_1%7D" alt="\mathrm{Laps_1}" eeimg="1"> ),( <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_2%7D" alt="\mathrm{Int_2}" eeimg="1"> , <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_2%7D" alt="\mathrm{Laps_2}" eeimg="1"> ),( <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_3%7D" alt="\mathrm{Int_3}" eeimg="1"> , <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_3%7D" alt="\mathrm{Laps_3}" eeimg="1"> ), ...,( <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> , <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1"> ))</p><p data-pid="Q7QKEPr7">其中：</p><ul><li data-pid="BiaskESv"><img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_i%7D" alt="\mathrm{Int_i}" eeimg="1"> - 第 i 次重复前使用的间隔（范围在 1 到 800 之间），</li><li data-pid="YL5Gw76Y"><img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_i%7D" alt="\mathrm{Laps_i}" eeimg="1"> - 在第 i 次重复过程中，遗忘的次数（范围从 0 到 40），</li><li data-pid="7is3hbYT"> n - 总重复次数（范围从 3 到 20）。</li></ul><p data-pid="NJhwazMV">4. 找到公式所描述的函数 f 和 g：</p><p data-pid="_Z6ppDW5"> S(1)=S1<br> S(n)=f(S(n-1), <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> , <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1"> )<br> Laps(n)=g(S(n-1), <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> )</p><p data-pid="mBiMvAN8">其中：</p><ul><li data-pid="3dOUCvvR"> S(n) - 与第 n 次重复后的记忆强度相对应的任何变量（比较<a href="http://link.zhihu.com/?target=http%3A//super-memory.com/english/ol/ol_memory.htm" class=" wrap external" target="_blank" rel="nofollow noreferrer">第 10 章</a>），</li><li data-pid="HXEMudAZ"><img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> - 第 n 次重复前使用的间隔；取自间歇学习期间收集的数据，</li><li data-pid="t05xmgR9"><img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1"> - 在第 n 次重复中的遗忘数量；取自间歇学习期间收集的数据，</li><li data-pid="ox-fztCM"> Laps(n) - 对第 n 次重复中记忆遗忘数量的估计（它应该与 <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1"> 相对应）</li><li data-pid="VSZ0T3TV"> S1 - 一个常数，</li></ul><p data-pid="05DuIpYG">使函数 Dev 最小化：</p><p data-pid="Zhxrkucb">Dev=sqrt(( <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BDev_1%7D" alt="\mathrm{Dev_1}" eeimg="1"> + <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BDev_2%7D" alt="\mathrm{Dev_2}" eeimg="1"> + ... + <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BDev_%7B161%7D%7D" alt="\mathrm{Dev_{161}}" eeimg="1">)/RepNo)<br><img src="https://www.zhihu.com/equation?tex=%5Cmathrm+Dev_i" alt="\mathrm Dev_i" eeimg="1"> =sqr(Laps(1)- <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_1%7D" alt="\mathrm{Laps_1}" eeimg="1">)+sqr(Laps(2)- <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_2%7D" alt="\mathrm{Laps_2}" eeimg="1">)+ ... +sqr(Laps(n)- <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1">))</p><p data-pid="eAfDIGoI">其中：</p><ul><li data-pid="hnADVyx9"> Dev - 描述函数 f 和 g 输出值之间差值的函数，值会在间歇学习期间收集（它反映了数据在实验和理论预测之间的差）</li><li data-pid="8GdSMYMS"> RepNo - 全部页面上的重复次数总和</li><li data-pid="XKlnmDUf"><img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BDev_i%7D" alt="\mathrm{Dev_i}" eeimg="1"> - 函数Dev的分项，对应第i页的Dev.</li><li data-pid="NB9P_2HT"> Laps(j) - 使用函数f和g，分别对第i页和第j次重复计算的遗忘数量，</li><li data-pid="qxHQ2RuO"><img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_j%7D" alt="\mathrm{Laps_j}" eeimg="1"> - 第 i 页、第 j 次重复时的遗忘数量；取自间歇学习期间收集的数据，</li><li data-pid="uAYUj_zF">sqrt(x) - x 的平方根，</li><li data-pid="z0mAU0Qy">sqr(x) - x 的平方。<br> </li></ul><p data-pid="GMxCAZXr">请注意，只有当函数 f 和 g 简单且定义参数有限时（如 a*ln()+b or a*exp()+b 等），才会有生物学上的思考价值。否则，人们总是可以构建一个巨大的、无意义的公式来自动将 Dev 归零。</p><hr><h2>解决间歇学习的问题</h2><hr><p data-pid="mBLtHdhi">存档警告：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用文字档案？</a></p><p data-pid="yZFELGDl">这段文字是《<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">优化学习</a>》 作者：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a> (1990) 的一部分</p><h3><b>11.2. 间歇学习的解决方案</b></h3><p data-pid="wKjeDLGv">在搜索使<i>Dev</i>值最小的函数f和g时，我用的是 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning_%281990%29%3A_References" class=" wrap external" target="_blank" rel="nofollow noreferrer">Wozniak, 1988b</a> 中描述的最小化的数值算法（<i>一种在可行区域内寻找函数局部最大值的新算法。可信论文</i>）。</p><p data-pid="1HiXrcVk">搜索中使用实例函数如下：</p><p data-pid="CYUQnBy_">S(1)=x[1]<br>S(n)=x[2]* <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1">*exp(- <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1">*x[3])+x[4])<br>Laps(n)=x[5]*(1-exp(- <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> /S(n-1)))</p><p data-pid="KIFpeHbS">其中：</p><ul><li data-pid="7iseLue9"> x[i] - 由最小化程序计算的变量，</li><li data-pid="ErHQxVJU"> S(n)、Laps(n)、<img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1"> 和<img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> - 如 11.1 中的定义</li></ul><p data-pid="iuxvniYR">注意，描述 S(n) 的函数 f 不使用 S(n-1) 作为它的参数（问题的表述允许，但不要求在先前强度的基础上计算新的强度）。</p><p data-pid="_J5XmR_v">为了保持简易度和节省时间，我设定了在最小化过程中使用 12 个变量的限制。</p><p data-pid="xM2w-4vJ">我测试了大量的数学函数，这些函数是根据有关记忆的明显直觉构建的（例如，随着时间的推移，页面遗忘的数量会增加）。</p><p data-pid="97NclELb">其中包括指数型、对数型、幂型、双曲线型、S 型、钟型、多项式及一些可能的组合。</p><p data-pid="OdkDRqNy">在大多数情况下，最小化程序将 Dev 的值减少到 3 以下，函数 f 和 g 的形状类似，与它们的性质独立。</p><p data-pid="J2sfvvBP">使用少于 12 个变量得到的 Dev 的最小值是 2.887241。</p><p data-pid="vk3vOdgk">函数 f 和 g 如下：</p><div class="highlight"><pre><code class="language-text"><span></span>constant S(1)=0.2104031;

function Sn(Intn,Lapsn,S(n-1));
begin
    S(n):=0.4584914*(Intn+1.47)*exp(-0.1549229*Lapsn-0.5854939)+0.35;
    if Lapsn=0 then
        if S(n-1)&gt;In then
            S(n):=S(n-1)*0.724994
        else 
            S(n):=Intn*1.1428571;
end;

function Lapsn(Intn,S(n-1));
var quot;
begin
    quot:=(Intn-0.16)/(S(n-1)-0.02)+1.652668;
    Lapsn:=-0.0005408*quot*quot+0.2196902*quot+0.311335;
end;
</code></pre></div><p data-pid="DPMpD5dZ">在不显著改变Dev的值的情况下，这些函数可以很容易地转换为以下形式：</p><p data-pid="bSTe5UHf">S(1)=1<br>for <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> &gt;S(n-1): S(n)=1.5* <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1">*exp(-0.15* <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_n%7D" alt="\mathrm{Laps_n}" eeimg="1">)+1<br>Laps(n)= <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1">/S(n-1)<br> <br> 请注意：</p><ul><li data-pid="jwwVfIsU"> 只要操作没有明显影响 Dev 的值，函数中的特定元素就会被删除或四舍五入，</li><li data-pid="cPO0MyYa"> 记忆强度进行了重新缩放，使其可以被解释为一个间隔，其中<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Lapse" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘</a>数量等于 1，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>等于 2.5%（一页有 40 个项目，1/40=2.5%），</li><li data-pid="bMnUBBBb"> 仅当 Intn 不小于 S(n-1) 时，强度公式才有效。这是因为，如果<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Lapse" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘</a>的数量很低，必须使用 S(n-1) 的值来计算 S(n)，例如 Intn &lt;= S(n-1): S(n)=S(n-1)*(1+0.5/1-exp(S(n-1))*(1-exp(- <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1">)))</li><li data-pid="qb_CiSu5"> 这些公式不能用于描述间隔比最优间隔长很多的过程。这是因为对于 <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BInt_n%7D" alt="\mathrm{Int_n}" eeimg="1"> -&gt;∞，Laps(n) 的值超过 100%，</li><li data-pid="PKzB0F6G"> 该公式描述了集体项目的学习，其特点是 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/E-Factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">E-系数</a>的分布或多或少地均匀。因此，它没能普遍用在难度可变的项目。</li></ul><p data-pid="XMaa33jT"> 就目前而言，上述公式构成了对间歇学习过程的最佳描述，以后将被称为间歇学习模型（简称 IL 模型）</p><hr><h2>基于间歇学习模型的模拟试验</h2><p data-pid="3XUnNf0m">有了上面发现的公式，我可以进行一系列的模拟实验，帮助我回答许多关于记忆在不同情况下的行为的假设情景。这些模拟实验影响了 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo</a> 之后多年的进展。特别是，从 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo_6" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo 6</a>（1991 ）开始，工作量和<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retention" class=" wrap external" target="_blank" rel="nofollow noreferrer">保留率</a>之间的权衡在优化学习方面起到了重要作用。直到今天，为学习提供指导标准的是<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>（或<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retrievability" class=" wrap external" target="_blank" rel="nofollow noreferrer">可提取性</a>），而不是在<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Recall" class=" wrap external" target="_blank" rel="nofollow noreferrer">回忆</a>水平较低时可能出现的、直观自然的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Stability_increase" class=" wrap external" target="_blank" rel="nofollow noreferrer">记忆稳定性增长</a>。设定记忆<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Lapse" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘</a>水平起到了下面<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>的作用。</p><hr><p data-pid="vrRfNBS4">存档警告：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用文字档案？</a></p><p data-pid="5hkE2rF3">这段文字选自《<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">优化学习</a>》 ，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a> (1990) 著</p><h3><b>11.4. 间歇学习模型的验证</b></h3><p data-pid="oeyeL-5B">为了验证间歇学习模型与 SuperMemo 理论的一致性，让我们尝试计算出分散重复的最优间隔。</p><p data-pid="l63Tg5RB">最优间隔由<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Lapse" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘</a>的数量达到选定值 Lapso 的时刻确定。</p><p data-pid="l4E-GFwy">算法如下：</p><p data-pid="I_HfyRgz">1. i:=1</p><p data-pid="6HIvaf-b">2. S(i):=1</p><p data-pid="eREVkTKw">3. 找到 Int(i+1)，使 Laps(i+1) 等于 <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_o%7D" alt="\mathrm{Laps_o}" eeimg="1"> . 使用公式：</p><p data-pid="tuz09y6J">Int(n)= <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_o%7D" alt="\mathrm{Laps_o}" eeimg="1"> *S(n-1) (取自 IL 模型)</p><p data-pid="nOQ7Fj3_">其中：</p><p data-pid="vbmy_jFI">Int(n) 表示第 n-1 个最优间隔。</p><p data-pid="AaIr_Wie">4. i:=i+1</p><p data-pid="9vb9u3Ou">5. S(i):=1.5*Int(i)*exp(-0.15*<img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_o%7D" alt="\mathrm{Laps_o}" eeimg="1">)+1（取自 IL 模型）</p><p data-pid="UfXpMkVY">6. goto 3</p><p data-pid="9W9RjlG-">如果 Lapso 等于 2.5（<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a> 6.25%），而且间歇学习模型参数相同，那么可以观察到惊人的对应关系（比较第 16 页<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Birthday_of_SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">第 3.1 章</a>介绍的实验）：</p><ul><li data-pid="WZ7h_KFB"> Rep - 重复的数量</li><li data-pid="i-QAP3F_"> 间隔 - 重复前的最优间隔，在 IL 模型的基础上由 <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_o%7D" alt="\mathrm{Laps_o}" eeimg="1"> =2.5 确定，</li><li data-pid="or9VchXY"> 系数 - 最优系数，等于最优间隔除以上一次的最优间隔，</li><li data-pid="n6sPRtbG"> SM-0 - 在得出 SM-0 算法的实验的基础上计算出的最优间隔</li></ul><table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"><tbody><tr><th>Rep</th><th>间隔</th><th>系数</th><th>SM-0</th></tr><tr><td>2</td><td>1.8</td><td></td><td>1</td></tr><tr><td>3</td><td>7.8</td><td>4.36</td><td>7</td></tr><tr><td>4</td><td>16.8</td><td>2.15</td><td>16</td></tr><tr><td>5</td><td>30.4</td><td>1.80</td><td>35</td></tr><tr><td>6</td><td>50.4</td><td>1.66</td><td></td></tr><tr><td>7</td><td>80.2</td><td>1.59</td><td></td></tr><tr><td>8</td><td>124</td><td>1.55</td><td></td></tr><tr><td>9</td><td>190</td><td>1.53</td><td></td></tr><tr><td>10</td><td>288</td><td>1.52</td><td></td></tr><tr><td>11</td><td>436</td><td>1.51</td><td></td></tr><tr><td>12</td><td>654</td><td>1.50</td><td></td></tr><tr><td>13</td><td>981</td><td>1.50</td><td></td></tr><tr><td>14</td><td>1462</td><td>1.49</td><td></td></tr><tr><td>15</td><td>2179</td><td>1.49</td><td></td></tr><tr><td>16</td><td>3247</td><td>1.49</td><td></td></tr><tr><td>17</td><td>4838</td><td>1.49</td><td></td></tr><tr><td>18</td><td>7209</td><td>1.49</td><td></td></tr></tbody></table><p data-pid="e7D6V-iV">显然，这种确切的对应关系，在某种程度上是一种巧合，因为导致制定 SM-0 算法的实验并不是那么敏感。</p><p data-pid="4yo01YpG">值得注意的是，<b><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">最优系数</a>有逐步降低的趋势！</b>这一事实似乎证实了最近一系列观察，这些观察基于对<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-5" class=" wrap external" target="_blank" rel="nofollow noreferrer">SM-5 算法</a> 中使用的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/OF_matrix" class=" wrap external" target="_blank" rel="nofollow noreferrer">最优系数矩阵</a>的分析。</p><p data-pid="tYNEtkkv">如果 Lapso 等于4（<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a> 10%，如<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-5" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-5</a>），那么<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">最优系数</a>的序列就类似于<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-5" class=" wrap external" target="_blank" rel="nofollow noreferrer">算法 SM-5</a>中 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/OF_matrix" class=" wrap external" target="_blank" rel="nofollow noreferrer">OF 矩阵</a>的一列。同时，知识保留几乎与 SM-5 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Database" class=" wrap external" target="_blank" rel="nofollow noreferrer">数据库</a>中的知识保留理想地匹配。</p><table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"><tbody><tr><th>Rep</th><th>间隔</th><th>保留率</th><th>系数</th></tr><tr><td>2</td><td>3</td><td>93.21678</td><td></td></tr><tr><td>3</td><td>16</td><td>93.80946</td><td>4.89</td></tr><tr><td>4</td><td>43</td><td>93.97184</td><td>2.74</td></tr><tr><td>5</td><td>102</td><td>94.04083</td><td>2.39</td></tr><tr><td>6</td><td>232</td><td>94.06886</td><td>2.27</td></tr><tr><td>7</td><td>517</td><td>94.08418</td><td>2.23</td></tr><tr><td>8</td><td>1138</td><td>94.09256</td><td>2.20</td></tr><tr><td>9</td><td>2502</td><td>94.09737</td><td>2.20</td></tr><tr><td>10</td><td>5481</td><td>94.09967</td><td>2.19</td></tr></tbody></table><p data-pid="cetv0ix0"><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retention" class=" wrap external" target="_blank" rel="nofollow noreferrer">保留率</a>是将最优过程中每天的保留率求平均值得到的</p><p data-pid="38x6SctA">R=(R(1)+R(2)+...+R(n))/n</p><p data-pid="aPRpzIOA">R(d)=100-2.5*Laps(d-dlr)</p><p data-pid="aGDPl_7g">其中：</p><ul><li data-pid="o17_TJwx"> R - 平均保留率</li><li data-pid="guQsK29w"> R(d) - 学习过程中第 d 天的保留率</li><li data-pid="Qszr1wwr"> Laps(Int) - 间隔 I 天后的期望遗忘数量</li><li data-pid="ilpCTir3"> dlr - 最后一次重复的日期</li></ul><hr><h2>工作量与保留率的权衡</h2><p data-pid="k5amLmGd">尽管模型使用了异质材料，有些不准确的地方，但对于<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>如何影响学习所需时间，也能可靠地得出结论。这些观察结果经受住了时间的考验：</p><hr><p data-pid="Z0yWFTbN">档案警告：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用文字档案？</a></p><p data-pid="cJ-P6PBw">这段文字是《<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">优化学习</a>》 作者：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a> (1990) 的一部分</p><p data-pid="HRiaObZ5">通过比较通过间歇学习模型计算的保留率和工作量数据，可以得出非常有趣的结论：</p><ul><li data-pid="fjom_VNq"><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">指数</a> - 遗忘指数（ <img src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BLaps_o%7D" alt="\mathrm{Laps_o}" eeimg="1"> *2.5）确定了时间最优学习的最优间隔，其中使用 IL 模型安排学习</li><li data-pid="iy6go6mS"><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retention" class=" wrap external" target="_blank" rel="nofollow noreferrer">保留率</a> - 在给定<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>得到的总体保留率（在 10,000 天后计算）</li><li data-pid="_yIylqA3"> 重复次数 - 在给定<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>下，在实验过程的前 10,000 天安排的重复次数，</li><li data-pid="brGVAMqf"><a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimal_factor" class=" wrap external" target="_blank" rel="nofollow noreferrer">系数</a> - 最优系数的渐近值（取自该过程的第 10000 天）</li></ul><table data-draft-node="block" data-draft-type="table" data-size="normal" data-row-style="normal"><tbody><tr><th>指数</th><th>保留率</th><th>重复次数</th><th>系数</th></tr><tr><td>2.5</td><td>97.76</td><td>两天一次</td><td>1.0000</td></tr><tr><td>4.5</td><td>96.88</td><td>65</td><td>1.0300</td></tr><tr><td>5.0</td><td>96.64</td><td>30</td><td>1.1600</td></tr><tr><td>5.5</td><td>96.39</td><td>22</td><td>1.3000</td></tr><tr><td>6.25</td><td>96.01</td><td>17</td><td>1.4900</td></tr><tr><td>7.5</td><td>95.37</td><td>13</td><td>1.7700</td></tr><tr><td>10.0</td><td>94.10</td><td>10</td><td>2.1900</td></tr><tr><td>12.5</td><td>92.78</td><td>9</td><td>2.4700</td></tr></tbody></table><p data-pid="CZYMcitp">图 11.2 表明，用于确定最优间隔的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>应落在 5-10% 的范围内。</p><figure data-size="normal"><img src="https://pica.zhimg.com/v2-6900213aaa3c21d197501288c638902c_720w.jpg?source=3af55fa1" data-caption="" data-size="normal" data-rawwidth="1096" data-rawheight="1198" class="origin_image zh-lightbox-thumb" width="1096" data-original="https://pic2.zhimg.com/v2-6900213aaa3c21d197501288c638902c_720w.jpg?source=3af55fa1"></figure><blockquote data-pid="XUO3RaVk">图 11.2 工作量-保留率的权衡：一方面，如果<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>低于 5%，那么工作量就会急剧增加，而不会对保留率产生实质性影响。另一方面，如果<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>超过 10%，工作量几乎没有变化，而保留率却稳步下降。显然，工作量-保留率的权衡直接对应于习得率和保留率之间的妥协。通过增加时间的可用性 X 倍（通过减少工作量 X 倍），可以增加习得率 X 倍（比较<a href="http://link.zhihu.com/?target=http%3A//super-memory.com/articles/theory.htm" class=" wrap external" target="_blank" rel="nofollow noreferrer">第 5 章</a>）。请注意，在这个模型中，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>和<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Retention" class=" wrap external" target="_blank" rel="nofollow noreferrer">保留率</a>的关系几乎是线性的。（来源：《<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">学习优化</a>》：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Search_for_a_universal_memory_formula" class=" wrap external" target="_blank" rel="nofollow noreferrer">间歇学习模型</a>，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a>, 1990)</blockquote><p data-pid="vQFGh8iD">另一重要观察来自使记忆强度增长最大化的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>的计算过程</p><p data-pid="y6jDuZwr">由间歇学习模型可得</p><p data-pid="KPZWJuVM">S(n)=1.5*Laps(n)*S(n-1)*exp(-0.15*Laps(n))+1</p><p data-pid="KKYt_23y">对变量 Laps(n) 进行微分后，我们得到：</p><p data-pid="ir94tfOM">S'(n)=1.5*S(n-1)*exp(-0.15*Laps(n))*(1-0.15*Laps(n))</p><p data-pid="N_yN3p3v">最后，令其等于  0，我们得到：</p><p data-pid="smVZJ_8F">Laps(n)=7.8</p><p data-pid="owdxKCYI">这相当于<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>等于 20%！这样的遗忘指数得出的间隔，相当于遗忘指数等于 10% 确定的最优间隔的 2 倍（如<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-5" class=" wrap external" target="_blank" rel="nofollow noreferrer">SM-5 算法</a>）。然而，别忘了，工作量的唯一权衡因素是知识保留率而不是记忆强度。因此，上述发现并没有令<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Algorithm_SM-5" class=" wrap external" target="_blank" rel="nofollow noreferrer">SM-5 算法</a> 失效</p><hr><h2>结论：间歇学习模型</h2><p data-pid="9RpvJjZQ">该章结尾处得出的最终结论经受住了三十年的考验。只有<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_curve" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘曲线</a>是非指数形状的说法是不准确的。这是因为这个模型是基于各种性质不同数据建立的，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Exponential_nature_of_forgetting" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘的指数性质</a>不可显现出来。</p><hr><p data-pid="Vz4tFlYw">档案警告：<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Why_use_literal_archives%253F" class=" wrap external" target="_blank" rel="nofollow noreferrer">为什么使用文字档案？</a></p><p data-pid="f4KUipbF">这段文字选自《<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Optimization_of_learning" class=" wrap external" target="_blank" rel="nofollow noreferrer">优化学习</a>》，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Piotr_Wozniak" class=" wrap external" target="_blank" rel="nofollow noreferrer">Piotr Wozniak</a> (1990) 著</p><h3><b>临时摘要</b></h3><ul><li data-pid="cbOTb2wR"> 构建了间歇学习模型，从而能对于不同的重复计划估计其知识保留率</li><li data-pid="9Cbg5jqn"> 该模型确凿地说明，<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_curve" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘曲线</a>不是指数型的 [2018 评论：错误的结论：对比<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Exponential_nature_of_forgetting" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘的指数性质</a>]</li><li data-pid="tOVQxUaM"> 该模型与实验数据吻合良好</li><li data-pid="UtLcqBVo"> 该模型能以惊人精度求出最优间隔和知识保留率的近似值，而这两个变量是 SuperMemo 模型所隐含的。</li><li data-pid="k9hLV7Om"> 该模型表明，最优系数在随着重复减少，并渐进接近最终值</li><li data-pid="hJqR-fF3"> 该模型表明，最节省学习时间的<a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/Forgetting_index" class=" wrap external" target="_blank" rel="nofollow noreferrer">遗忘指数</a>的理想值应落在 5% 至 10% 之间</li><li data-pid="vEqjRp_F"> 该模型表明，遗忘指数与知识保留率几乎呈线性关系</li><li data-pid="wcXotwUA"> 该模型说明，当间隔比 <a href="http://link.zhihu.com/?target=https%3A//supermemo.guru/wiki/SuperMemo" class=" wrap external" target="_blank" rel="nofollow noreferrer">SuperMemo 方法</a>中使用的间隔长约 2 倍时，记忆强度的增幅最大。这相当于遗忘指数等于 20%<br> </li></ul><hr><p data-pid="kXRWrK48">还是那个flag，争取年底把坑填完！</p>